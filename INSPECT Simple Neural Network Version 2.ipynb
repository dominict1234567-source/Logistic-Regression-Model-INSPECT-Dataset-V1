{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c47b23c-62fe-4083-9653-b6ea06ebecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT IS RUNNING!\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\n",
      "Combined rows: 10502789  Files loaded: 5\n",
      "\n",
      "Number of CSV files per subject:\n",
      "source_file\n",
      "1    946\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "\n",
      "Unique subjects: 946\n",
      "Dataset size (pre-cap): 62730  Positives: 1464  Negatives: 61266\n",
      "Dataset size (post-cap): 62730  Positives: 1464  Negatives: 61266\n",
      "\n",
      "Label horizon: 7..30 days\n",
      "Negative buffer before cluster starts: 7 days\n",
      "\n",
      "Fold data_0.csv: AUROC=0.727  AUPRC=0.047  n=12193  pos=232\n",
      "Fold data_1.csv: AUROC=0.732  AUPRC=0.063  n=12669  pos=355\n",
      "Fold data_2.csv: AUROC=0.781  AUPRC=0.090  n=12741  pos=295\n",
      "Fold data_3.csv: AUROC=0.758  AUPRC=0.067  n=11346  pos=284\n",
      "Fold data_4.csv: AUROC=0.773  AUPRC=0.062  n=13781  pos=298\n",
      "\n",
      "================== POOLED OOF RESULTS ==================\n",
      "Test rows: 62730  Positives: 1464  Negatives: 61266\n",
      "Prevalence: 0.0233\n",
      "AUROC: 0.755\n",
      "AUPRC: 0.064\n",
      "Brier: 0.1592\n",
      "Predicted prob quantiles [min,50%,90%,99%,max]: [0.     0.1134 0.7581 0.9763 1.    ]\n",
      "\n",
      "=== Classification report @ threshold = 0.5 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     0.765     0.862     61266\n",
      "           1      0.053     0.548     0.096      1464\n",
      "\n",
      "    accuracy                          0.760     62730\n",
      "   macro avg      0.519     0.657     0.479     62730\n",
      "weighted avg      0.964     0.760     0.844     62730\n",
      "\n",
      "\n",
      "=== Best-F1 threshold from PR curve ===\n",
      "Chosen threshold: 0.8255  (F1=0.122, Precision=0.082, Recall=0.244)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.935     0.957     61266\n",
      "           1      0.082     0.244     0.122      1464\n",
      "\n",
      "    accuracy                          0.918     62730\n",
      "   macro avg      0.531     0.589     0.540     62730\n",
      "weighted avg      0.960     0.918     0.938     62730\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"IT IS RUNNING!\")\n",
    "\n",
    "\"\"\"\n",
    "Neural Network temporal cluster-start prediction — V2 \n",
    "\n",
    "Pipeline ::::\n",
    "\n",
    "1) Load MULTIPLE CSVs (data_0.csv ... data_4.csv), tag each row with its file of origin (source_file),\n",
    "   and concatenate into one event-level dataframe.\n",
    "2) For each patient (subject_id):\n",
    "   a) Identify \"clusters\" of dense measurement activity (runs of calendar days with small day gaps).\n",
    "   b) Construct index days using UPDATED rule:\n",
    "      - pick ONLY ONE index day per quiet spell (first eligible day after a sparse gap)\n",
    "      - skip days that fall inside clusters\n",
    "      - drop index days that fall within N days before a future cluster start\n",
    "   c) Build UPDATED features at each index:\n",
    "      - value features for 4 codes (SBP, HR, glucose, HbA1c): last, days since last, window median/IQR/count\n",
    "      - richer dynamics: delta(last - window_median), delta(last - previous), last relative to patient baseline IQR\n",
    "      - cadence/behaviour features: measurement rhythm + cluster history\n",
    "   d) Assign label:\n",
    "      - positive if a cluster START occurs within [label_start_days .. label_end_days] after the index time\n",
    "3) Add missingness indicator columns for all features before imputation.\n",
    "4) Leave-one-CSV-out cross-validation:\n",
    "   - fold = one held-out source_file\n",
    "   - impute (median) + scale (StandardScaler) using TRAIN fold only\n",
    "   - oversample minority class in TRAIN fold only\n",
    "   - fit MLPClassifier(1 hidden layer), predict_proba on the held-out fold\n",
    "5) Pool out-of-fold predictions and print one combined evaluation.\n",
    "\n",
    "Improvements from old model (V1) to new model(V2) ::\n",
    "\n",
    "V1 (older FIXED MLP):\n",
    "- Label window: 8..90 days after index\n",
    "- Index-day construction: could create multiple indices during sparse periods\n",
    "- Simpler features: last/days_since_last + basic window stats\n",
    "- Minimal behavioural/cadence context\n",
    "- No explicit ambiguous-negative removal near cluster starts\n",
    "\n",
    "V2 (this script):\n",
    "- Label window updated to 7..30 days (DEFAULT_LABEL_START_DAYS..DEFAULT_LABEL_END_DAYS)\n",
    "- New ambiguous-negative buffer: drop indices within 7 days before any future cluster start\n",
    "- New index policy: One index per quiet spell after sparse gap (reduces correlated samples)\n",
    "- Expanded feature set: deltas + patient baseline-relative features + cadence/cluster-history features\n",
    "- Adds missingness indicator features prior to imputation\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "# Default input CSVs (expects the same schema in each file)\n",
    "DEFAULT_CSV_PATHS = [\n",
    "    \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\",\n",
    "    \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\",\n",
    "    \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\",\n",
    "    \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\",\n",
    "    \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\",\n",
    "]\n",
    "\n",
    "# Feature lookback window for summary statistics\n",
    "DEFAULT_LOOKBACK_DAYS = 90\n",
    "\n",
    "# Cluster logic:\n",
    "# - sparse_gap_days: minimum quiet gap required before a dense run can be considered a \"new cluster\"\n",
    "# - dense_gap_days: max allowed gap between consecutive observation days within a cluster\n",
    "# - min_cluster_days: minimum number of unique activity days required for a cluster\n",
    "DEFAULT_SPARSE_GAP_DAYS = 6\n",
    "DEFAULT_DENSE_GAP_DAYS = 2\n",
    "DEFAULT_MIN_CLUSTER_DAYS = 4\n",
    "\n",
    "# Restrict clustering and measurement features to this table name if present\n",
    "DEFAULT_CLUSTER_FROM_TABLE = \"measurement\"\n",
    "\n",
    "# Updated label horizon\n",
    "DEFAULT_LABEL_START_DAYS = 7\n",
    "DEFAULT_LABEL_END_DAYS = 30\n",
    "\n",
    "# UPDATED: drop ambiguous negatives that are too close to a future cluster start\n",
    "DEFAULT_NEGATIVE_BUFFER_BEFORE_CLUSTER_DAYS = 7\n",
    "\n",
    "# Optional: cap number of negatives per subject (helps extreme imbalance); 0 = OFF\n",
    "DEFAULT_MAX_NEG_PER_SUBJECT = 0\n",
    "\n",
    "# Clinical codes used as features\n",
    "CODE_SBP = \"LOINC/8480-6\"          # systolic BP\n",
    "CODE_HR = \"LOINC/8867-4\"           # heart rate\n",
    "CODE_GLUCOSE = \"SNOMED/271649006\"  # glucose\n",
    "CODE_HBA1C = \"SNOMED/271650006\"    # HbA1c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Oversampling is used because sklearn MLPClassifier doesnt accept sample_weight.\n",
    "# IMPORTANT: oversampling happens ONLY in TRAIN folds (never in test fold). \n",
    "def oversample_minority(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Randomly oversample minority class to match majority count.\n",
    "    X: numpy array (n_samples, n_features)\n",
    "    y: numpy array (n_samples,)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "\n",
    "    # If only one class exists, nothing we can resample\n",
    "    if len(idx_pos) == 0 or len(idx_neg) == 0:\n",
    "        return X, y\n",
    "\n",
    "    # Identify minority / majority\n",
    "    if len(idx_pos) < len(idx_neg):\n",
    "        idx_min, idx_maj = idx_pos, idx_neg\n",
    "    else:\n",
    "        idx_min, idx_maj = idx_neg, idx_pos\n",
    "\n",
    "    # Add enough samples of the minority to match the majority size\n",
    "    n_to_add = len(idx_maj) - len(idx_min)\n",
    "    if n_to_add <= 0:\n",
    "        return X, y\n",
    "\n",
    "    extra_idx = rng.choice(idx_min, size=n_to_add, replace=True)\n",
    "    X_bal = np.vstack([X, X[extra_idx]])\n",
    "    y_bal = np.concatenate([y, y[extra_idx]])\n",
    "\n",
    "    # Shuffle so duplicates are not in one contiguous block\n",
    "    perm = rng.permutation(len(y_bal))\n",
    "    return X_bal[perm], y_bal[perm]\n",
    "\n",
    "\n",
    "# Optional: cap negatives per subject to reduce extreme imbalance / correlated negatives.\n",
    "# NOTE: This is applied AFTER index/feature building, BEFORE CV splitting.\n",
    "def cap_negatives_per_subject(data: pd.DataFrame, max_neg_per_subject: int, seed: int = 42) -> pd.DataFrame:\n",
    "    if max_neg_per_subject is None or max_neg_per_subject <= 0:\n",
    "        return data\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    kept_rows = []\n",
    "    for sid, g in data.groupby(\"subject_id\", sort=False):\n",
    "        pos = g[g[\"label\"] == 1]\n",
    "        neg = g[g[\"label\"] == 0]\n",
    "        if len(neg) > max_neg_per_subject:\n",
    "            idx = rng.choice(neg.index.to_numpy(), size=max_neg_per_subject, replace=False)\n",
    "            neg = neg.loc[idx]\n",
    "        kept_rows.append(pd.concat([pos, neg], axis=0))\n",
    "\n",
    "    return (\n",
    "        pd.concat(kept_rows, ignore_index=True)\n",
    "        .sort_values([\"subject_id\", \"index_time\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Parses mixed-format datetime strings in a pandas Series into datetimes.\n",
    "def parse_mixed_datetime(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\")\n",
    "    mask_slash = s.str.contains(\"/\", na=False)\n",
    "\n",
    "    parsed_slash = pd.to_datetime(\n",
    "        s.where(mask_slash),\n",
    "        format=\"%d/%m/%Y %H:%M\",\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "    parsed_other = pd.to_datetime(\n",
    "        s.where(~mask_slash),\n",
    "        errors=\"coerce\",\n",
    "        dayfirst=False,\n",
    "    )\n",
    "    return parsed_slash.fillna(parsed_other)\n",
    "\n",
    "\n",
    "# Loads a CSV, parses time + numeric_value, and sorts records by subject and time.\n",
    "# Also checks required columns exist.\n",
    "def load_csv_simple(path: str) -> pd.DataFrame:\n",
    "    print(\"Loading CSV from\", path)\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\", low_memory=False)\n",
    "\n",
    "    if \"time\" not in df.columns:\n",
    "        raise ValueError(f\"'time' column not found in {path}\")\n",
    "    if \"subject_id\" not in df.columns:\n",
    "        raise ValueError(f\"'subject_id' column not found in {path}\")\n",
    "\n",
    "    df[\"time\"] = parse_mixed_datetime(df[\"time\"])\n",
    "\n",
    "    # numeric_value may be missing for some event types → coerce safely\n",
    "    if \"numeric_value\" in df.columns:\n",
    "        df[\"numeric_value\"] = pd.to_numeric(df[\"numeric_value\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"numeric_value\"] = np.nan\n",
    "\n",
    "    # Stable sort for reproducibility\n",
    "    df = df.sort_values([\"subject_id\", \"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Identifies clusters of dense measurement activity at day granularity.\n",
    "# Cluster definition:\n",
    "# - collapse timestamps to unique calendar days\n",
    "# - dense run = consecutive days where day gaps <= dense_gap_days\n",
    "# - accept run as cluster if:\n",
    "#    (1) run_length >= min_cluster_days\n",
    "#    (2) preceding gap >= sparse_gap_days (or it's the first run)\n",
    "def find_clusters_for_patient(\n",
    "    patient_df: pd.DataFrame,\n",
    "    sparse_gap_days: int,\n",
    "    dense_gap_days: int,\n",
    "    min_cluster_days: int,\n",
    "    cluster_from_table: str | None,\n",
    "):\n",
    "    if cluster_from_table is not None and \"table\" in patient_df.columns:\n",
    "        chosen = patient_df[\n",
    "            (patient_df[\"table\"] == cluster_from_table) & (~patient_df[\"time\"].isna())\n",
    "        ].copy()\n",
    "        if chosen.empty:\n",
    "            chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    else:\n",
    "        chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "\n",
    "    # No valid timestamps → no clusters\n",
    "    if chosen.empty:\n",
    "        return []\n",
    "\n",
    "    # Work at day granularity\n",
    "    chosen[\"day\"] = chosen[\"time\"].dt.normalize()\n",
    "    unique_days = sorted(chosen[\"day\"].unique().tolist())\n",
    "\n",
    "    clusters = []\n",
    "    if len(unique_days) < min_cluster_days:\n",
    "        return clusters\n",
    "\n",
    "    # Gaps between adjacent observed days\n",
    "    day_diffs = []\n",
    "    for i in range(1, len(unique_days)):\n",
    "        day_diffs.append((unique_days[i] - unique_days[i - 1]).days)\n",
    "\n",
    "    # Scan dense runs\n",
    "    run_start_index = 0\n",
    "    for i, gap in enumerate(day_diffs, start=1):\n",
    "        if gap <= dense_gap_days:\n",
    "            continue\n",
    "\n",
    "        run_end_index = i - 1\n",
    "        run_length = run_end_index - run_start_index + 1\n",
    "\n",
    "        if run_length >= min_cluster_days:\n",
    "            preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "            if preceding_gap >= sparse_gap_days:\n",
    "                start_day = unique_days[run_start_index]\n",
    "                end_day = unique_days[run_end_index]\n",
    "                clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "        run_start_index = i\n",
    "\n",
    "    # Tail run\n",
    "    run_end_index = len(unique_days) - 1\n",
    "    run_length = run_end_index - run_start_index + 1\n",
    "    if run_length >= min_cluster_days:\n",
    "        preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "        if preceding_gap >= sparse_gap_days:\n",
    "            start_day = unique_days[run_start_index]\n",
    "            end_day = unique_days[run_end_index]\n",
    "            clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Convenience: expand cluster spans into a set of calendar days (inclusive).\n",
    "def build_cluster_day_set(clusters):\n",
    "    return {\n",
    "        pd.Timestamp(d).normalize()\n",
    "        for start_day, end_day, _ in clusters\n",
    "        for d in pd.date_range(start_day, end_day, freq=\"D\")\n",
    "    }\n",
    "\n",
    "\n",
    "# Convenience: list of cluster start days (day-normalized).\n",
    "def build_cluster_start_days(clusters):\n",
    "    return sorted([pd.Timestamp(s).normalize() for (s, _, _) in clusters])\n",
    "\n",
    "\n",
    "# UPDATED: treat index days that fall shortly BEFORE a future cluster start as ambiguous negatives.\n",
    "# If a cluster begins within buffer_days AFTER index_day, we drop that index.\n",
    "def is_too_close_to_future_cluster(index_day, cluster_start_days, buffer_days: int) -> bool:\n",
    "    if buffer_days <= 0 or not cluster_start_days:\n",
    "        return False\n",
    "    for s in cluster_start_days:\n",
    "        delta = (s - index_day).days\n",
    "        if 0 < delta <= buffer_days:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# UPDATED index construction (V2):\n",
    "# - ONE index day per quiet spell after sparse gap\n",
    "# - skip cluster days\n",
    "# - drop indices too close to future cluster starts (ambiguous-negative buffer)\n",
    "# - store (subject_id, index_time, index_day, source_file)\n",
    "def build_index_days_one_per_quiet_spell(\n",
    "    patient_df: pd.DataFrame,\n",
    "    clusters,\n",
    "    sparse_gap_days: int,\n",
    "    negative_buffer_before_cluster_days: int = 0,\n",
    "):\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        return []\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    cluster_day_set = build_cluster_day_set(clusters)\n",
    "    cluster_start_days = build_cluster_start_days(clusters)\n",
    "\n",
    "    day_rows = df[[\"day\"]].drop_duplicates().sort_values(\"day\").reset_index(drop=True)\n",
    "\n",
    "    index_list = []\n",
    "    prev_day = None\n",
    "    sid = patient_df[\"subject_id\"].iloc[0]\n",
    "\n",
    "    # \"eligible_open\" means we haven't yet taken an index for the current quiet spell\n",
    "    eligible_open = True\n",
    "\n",
    "    for i in range(len(day_rows)):\n",
    "        current_day = day_rows.loc[i, \"day\"]\n",
    "        in_cluster = current_day in cluster_day_set\n",
    "        gap_days = math.inf if prev_day is None else (current_day - prev_day).days\n",
    "\n",
    "        # A new quiet spell begins whenever we observe a sufficiently long gap\n",
    "        if gap_days >= sparse_gap_days:\n",
    "            eligible_open = True\n",
    "\n",
    "        # Take the FIRST non-cluster day in that new quiet spell\n",
    "        if eligible_open and (not in_cluster):\n",
    "            # Drop ambiguous negatives close to future cluster starts (buffer)\n",
    "            if is_too_close_to_future_cluster(\n",
    "                current_day,\n",
    "                cluster_start_days,\n",
    "                negative_buffer_before_cluster_days\n",
    "            ):\n",
    "                prev_day = current_day\n",
    "                continue\n",
    "\n",
    "            first_time = df.loc[df[\"day\"] == current_day, \"time\"].min()\n",
    "\n",
    "            # Preserve source_file (used for leave-one-CSV-out CV)\n",
    "            if \"source_file\" in df.columns:\n",
    "                src_candidates = df.loc[(df[\"day\"] == current_day) & (df[\"time\"] == first_time), \"source_file\"]\n",
    "                src_file = src_candidates.iloc[0] if len(src_candidates) > 0 else \"unknown\"\n",
    "            else:\n",
    "                src_file = \"unknown\"\n",
    "\n",
    "            index_list.append((sid, first_time, current_day, src_file))\n",
    "            eligible_open = False  # close this quiet spell; no more indices until next sparse gap\n",
    "\n",
    "        prev_day = current_day\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "\n",
    "# Robust IQR helper:\n",
    "# - returns NaN if too few points\n",
    "def robust_iqr(x: np.ndarray) -> float:\n",
    "    if len(x) < 4:\n",
    "        return np.nan\n",
    "    q1 = float(np.nanquantile(x, 0.25))\n",
    "    q3 = float(np.nanquantile(x, 0.75))\n",
    "    return q3 - q1\n",
    "\n",
    "\n",
    "# Patient baselines (computed once per patient):\n",
    "# - baseline median and IQR per code across all patient measurement history\n",
    "# Used to normalise \"last value\" relative to the patient’s typical range.\n",
    "def build_patient_baselines(meas_df: pd.DataFrame, codes):\n",
    "    out = {}\n",
    "    for code_val in codes:\n",
    "        vals = meas_df.loc[meas_df[\"code\"] == code_val, \"numeric_value\"].dropna().to_numpy(dtype=float)\n",
    "        if len(vals) == 0:\n",
    "            out[code_val] = (np.nan, np.nan)\n",
    "        else:\n",
    "            out[code_val] = (float(np.nanmedian(vals)), robust_iqr(vals))\n",
    "    return out\n",
    "\n",
    "\n",
    "# UPDATED value features per code:\n",
    "# - last value + days since last\n",
    "# - lookback window median/IQR/count\n",
    "# - delta(last - window_median)\n",
    "# - delta(last - previous value)\n",
    "# - normalised last relative to patient baseline IQR\n",
    "def build_features_for_index(\n",
    "    patient_df: pd.DataFrame,\n",
    "    index_time: pd.Timestamp,\n",
    "    lookback_days: int,\n",
    "    patient_baselines: dict,\n",
    "):\n",
    "    result = {}\n",
    "\n",
    "    meas_df = patient_df.copy()\n",
    "    if \"table\" in meas_df.columns:\n",
    "        meas_df = meas_df[meas_df[\"table\"] == \"measurement\"].copy()\n",
    "\n",
    "    lookback_start = index_time - pd.Timedelta(days=lookback_days)\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "\n",
    "    for code_val in codes:\n",
    "        col_prefix = code_val.replace(\"/\", \"_\")\n",
    "        code_df = meas_df[(meas_df[\"code\"] == code_val) & (~meas_df[\"time\"].isna())].copy()\n",
    "\n",
    "        # History up to index time\n",
    "        hist_df = code_df[code_df[\"time\"] <= index_time].sort_values(\"time\")\n",
    "\n",
    "        if hist_df.empty:\n",
    "            last_val = np.nan\n",
    "            last_days_since = np.nan\n",
    "            prev_val = np.nan\n",
    "        else:\n",
    "            last_val = hist_df.iloc[-1][\"numeric_value\"]\n",
    "            last_days_since = (index_time - hist_df.iloc[-1][\"time\"]).days\n",
    "            prev_val = hist_df.iloc[-2][\"numeric_value\"] if len(hist_df) >= 2 else np.nan\n",
    "\n",
    "        # Lookback window stats\n",
    "        window_df = code_df[(code_df[\"time\"] >= lookback_start) & (code_df[\"time\"] <= index_time)]\n",
    "        window_vals = window_df[\"numeric_value\"].dropna().to_numpy(dtype=float)\n",
    "\n",
    "        if len(window_vals) == 0:\n",
    "            window_median = np.nan\n",
    "            window_iqr = np.nan\n",
    "            window_count = 0\n",
    "        else:\n",
    "            window_median = float(np.nanmedian(window_vals))\n",
    "            window_count = int(len(window_vals))\n",
    "            window_iqr = robust_iqr(window_vals)\n",
    "\n",
    "        # Deltas\n",
    "        delta_last_median = (\n",
    "            np.nan if (np.isnan(last_val) or np.isnan(window_median)) else float(last_val - window_median)\n",
    "        )\n",
    "        delta_last_prev = (\n",
    "            np.nan if (np.isnan(last_val) or np.isnan(prev_val)) else float(last_val - prev_val)\n",
    "        )\n",
    "\n",
    "        # Baseline-relative normalisation\n",
    "        pat_med, pat_iqr = patient_baselines.get(code_val, (np.nan, np.nan))\n",
    "        if np.isnan(last_val) or np.isnan(pat_med) or np.isnan(pat_iqr) or pat_iqr == 0:\n",
    "            last_rel = np.nan\n",
    "        else:\n",
    "            last_rel = float((last_val - pat_med) / pat_iqr)\n",
    "\n",
    "        # Store features\n",
    "        result[f\"{col_prefix}_last\"] = last_val\n",
    "        result[f\"{col_prefix}_days_since_last\"] = last_days_since\n",
    "        result[f\"{col_prefix}_median_{lookback_days}d\"] = window_median\n",
    "        result[f\"{col_prefix}_iqr_{lookback_days}d\"] = window_iqr\n",
    "        result[f\"{col_prefix}_count_{lookback_days}d\"] = window_count\n",
    "        result[f\"{col_prefix}_delta_last_median_{lookback_days}d\"] = delta_last_median\n",
    "        result[f\"{col_prefix}_delta_last_prev\"] = delta_last_prev\n",
    "        result[f\"{col_prefix}_last_rel_patient_iqr\"] = last_rel\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# NEW cadence/behaviour features:\n",
    "# - measurement rhythm: days since last measurement day, last gap between measurement days\n",
    "# - activity counts: number of distinct measurement days in last 7/30/90 days\n",
    "# - cluster history: total clusters, days since last cluster start/end (before index day)\n",
    "def build_cadence_features(\n",
    "    patient_df: pd.DataFrame,\n",
    "    index_time: pd.Timestamp,\n",
    "    clusters,\n",
    "):\n",
    "    out = {}\n",
    "\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        out[\"any_days_since_last_meas\"] = np.nan\n",
    "        out[\"any_last_gap_days\"] = np.nan\n",
    "        for w in (7, 30, 90):\n",
    "            out[f\"any_count_days_{w}d\"] = 0\n",
    "        out[\"cluster_count_total\"] = 0\n",
    "        out[\"days_since_last_cluster_start\"] = np.nan\n",
    "        out[\"days_since_last_cluster_end\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    # Restrict cadence metrics to measurement table if present\n",
    "    if \"table\" in df.columns:\n",
    "        df = df[df[\"table\"] == \"measurement\"].copy()\n",
    "\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    if df.empty:\n",
    "        out[\"any_days_since_last_meas\"] = np.nan\n",
    "        out[\"any_last_gap_days\"] = np.nan\n",
    "        for w in (7, 30, 90):\n",
    "            out[f\"any_count_days_{w}d\"] = 0\n",
    "        out[\"cluster_count_total\"] = int(len(clusters)) if clusters else 0\n",
    "        out[\"days_since_last_cluster_start\"] = np.nan\n",
    "        out[\"days_since_last_cluster_end\"] = np.nan\n",
    "        return out\n",
    "\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    # Measurement days up to index time\n",
    "    days_upto = df.loc[df[\"time\"] <= index_time, \"day\"].dropna().unique()\n",
    "    days_upto = sorted(pd.to_datetime(days_upto))\n",
    "\n",
    "    if len(days_upto) == 0:\n",
    "        out[\"any_days_since_last_meas\"] = np.nan\n",
    "        out[\"any_last_gap_days\"] = np.nan\n",
    "    else:\n",
    "        last_day = days_upto[-1]\n",
    "        out[\"any_days_since_last_meas\"] = float((index_time.normalize() - last_day).days)\n",
    "        out[\"any_last_gap_days\"] = float((days_upto[-1] - days_upto[-2]).days) if len(days_upto) >= 2 else np.nan\n",
    "\n",
    "    # Distinct measurement-day counts in short/medium/long windows\n",
    "    idx_day = index_time.normalize()\n",
    "    for w in (7, 30, 90):\n",
    "        start = idx_day - pd.Timedelta(days=w)\n",
    "        mask = (df[\"day\"] >= start) & (df[\"day\"] <= idx_day)\n",
    "        out[f\"any_count_days_{w}d\"] = int(df.loc[mask, \"day\"].nunique())\n",
    "\n",
    "    # Cluster history features\n",
    "    out[\"cluster_count_total\"] = int(len(clusters)) if clusters else 0\n",
    "\n",
    "    last_start = np.nan\n",
    "    last_end = np.nan\n",
    "    if clusters:\n",
    "        starts = [s for (s, e, _) in clusters if pd.Timestamp(s) <= idx_day]\n",
    "        ends = [e for (s, e, _) in clusters if pd.Timestamp(e) <= idx_day]\n",
    "        if starts:\n",
    "            last_start = float((idx_day - max(starts)).days)\n",
    "        if ends:\n",
    "            last_end = float((idx_day - max(ends)).days)\n",
    "\n",
    "    out[\"days_since_last_cluster_start\"] = last_start\n",
    "    out[\"days_since_last_cluster_end\"] = last_end\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Positive if any cluster starts within [label_start_days .. label_end_days] after index_time.\n",
    "def label_index_row(index_time, clusters, label_start_days: int, label_end_days: int) -> int:\n",
    "    if not clusters:\n",
    "        return 0\n",
    "    horizon_start = (index_time + pd.Timedelta(days=label_start_days)).normalize()\n",
    "    horizon_end = (index_time + pd.Timedelta(days=label_end_days)).normalize()\n",
    "    for (start_time, _, _) in clusters:\n",
    "        if (start_time >= horizon_start) and (start_time <= horizon_end):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Choose threshold that maximises F1 from pooled PR curve (useful under class imbalance).\n",
    "def best_f1_threshold(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    thr = float(thresholds[best_idx]) if best_idx < len(thresholds) else 1.0\n",
    "    return thr, float(f1[best_idx]), float(precision[best_idx]), float(recall[best_idx])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Temporal cluster-start prediction (MLP) — V2 UPDATED\")\n",
    "    parser.add_argument(\"-f\", default=None, help=argparse.SUPPRESS)\n",
    "\n",
    "    # Data + temporal settings\n",
    "    parser.add_argument(\"--csv\", nargs=\"+\", default=DEFAULT_CSV_PATHS, help=\"One or more CSV file paths.\")\n",
    "    parser.add_argument(\"--lookback-days\", type=int, default=DEFAULT_LOOKBACK_DAYS)\n",
    "    parser.add_argument(\"--sparse-gap-days\", type=int, default=DEFAULT_SPARSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--dense-gap-days\", type=int, default=DEFAULT_DENSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--min-cluster-days\", type=int, default=DEFAULT_MIN_CLUSTER_DAYS)\n",
    "    parser.add_argument(\"--cluster-from-table\", default=DEFAULT_CLUSTER_FROM_TABLE)\n",
    "\n",
    "    # UPDATED label horizon controls (7..30 default)\n",
    "    parser.add_argument(\"--label-start-days\", type=int, default=DEFAULT_LABEL_START_DAYS)\n",
    "    parser.add_argument(\"--label-end-days\", type=int, default=DEFAULT_LABEL_END_DAYS)\n",
    "\n",
    "    # UPDATED: ambiguous-negative buffer (drop indices close to future cluster starts)\n",
    "    parser.add_argument(\n",
    "        \"--negative-buffer-before-cluster-days\",\n",
    "        type=int,\n",
    "        default=DEFAULT_NEGATIVE_BUFFER_BEFORE_CLUSTER_DAYS\n",
    "    )\n",
    "\n",
    "    # Optional imbalance control at dataset level (in addition to oversampling)\n",
    "    parser.add_argument(\"--max-neg-per-subject\", type=int, default=DEFAULT_MAX_NEG_PER_SUBJECT)\n",
    "\n",
    "    # MLP hyperparams (optional overrides)\n",
    "    parser.add_argument(\"--hidden-units\", type=int, default=12)\n",
    "    parser.add_argument(\"--max-iter\", type=int, default=500)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    # Allow \"--cluster-from-table none\" to disable table restriction\n",
    "    cluster_from_table = None if str(args.cluster_from_table).lower() == \"none\" else args.cluster_from_table\n",
    "\n",
    "    dfs = []\n",
    "    for path in args.csv:\n",
    "        p = Path(path).expanduser()\n",
    "        part = load_csv_simple(str(p))\n",
    "\n",
    "        # Tag each row with the CSV filename (defines CV folds)\n",
    "        part[\"source_file\"] = p.name\n",
    "        dfs.append(part)\n",
    "\n",
    "    df = (\n",
    "        pd.concat(dfs, ignore_index=True)\n",
    "        .sort_values([\"subject_id\", \"time\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Combined rows: {len(df)}  Files loaded: {len(dfs)}\")\n",
    "\n",
    "    # Diagnostic: how many files contribute to each subject (helps spot leakage)\n",
    "    leakage_check = df.groupby(\"subject_id\")[\"source_file\"].nunique().value_counts().sort_index()\n",
    "    print(\"\\nNumber of CSV files per subject:\")\n",
    "    print(leakage_check)\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    print(f\"Unique subjects: {df['subject_id'].nunique(dropna=True)}\")\n",
    "\n",
    "\n",
    "    rows = []\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "\n",
    "    for sid, patient in df.groupby(\"subject_id\", sort=False):\n",
    "        #Find clusters\n",
    "        clusters = find_clusters_for_patient(\n",
    "            patient,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            dense_gap_days=args.dense_gap_days,\n",
    "            min_cluster_days=args.min_cluster_days,\n",
    "            cluster_from_table=cluster_from_table,\n",
    "        )\n",
    "\n",
    "        # Compute patient baselines once per patient (median/IQR per code)\n",
    "        meas_for_baseline = patient.copy()\n",
    "        if \"table\" in meas_for_baseline.columns:\n",
    "            meas_for_baseline = meas_for_baseline[meas_for_baseline[\"table\"] == \"measurement\"].copy()\n",
    "        patient_baselines = build_patient_baselines(meas_for_baseline, codes)\n",
    "\n",
    "        #Build UPDATED indices (one per quiet spell, skip clusters, drop ambiguous negatives)\n",
    "        index_days = build_index_days_one_per_quiet_spell(\n",
    "            patient,\n",
    "            clusters,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            negative_buffer_before_cluster_days=args.negative_buffer_before_cluster_days,\n",
    "        )\n",
    "\n",
    "        #For each index: build features + label\n",
    "        for sid2, idx_time, idx_day, src_file in index_days:\n",
    "            feats = build_features_for_index(\n",
    "                patient,\n",
    "                idx_time,\n",
    "                lookback_days=args.lookback_days,\n",
    "                patient_baselines=patient_baselines,\n",
    "            )\n",
    "\n",
    "            cadence_feats = build_cadence_features(patient, idx_time, clusters)\n",
    "\n",
    "            label = label_index_row(\n",
    "                idx_time,\n",
    "                clusters,\n",
    "                label_start_days=args.label_start_days,\n",
    "                label_end_days=args.label_end_days,\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"subject_id\": sid2,\n",
    "                \"index_time\": idx_time,\n",
    "                \"index_day\": idx_day,\n",
    "                \"label\": int(label),\n",
    "                \"source_file\": src_file,\n",
    "            }\n",
    "            row.update(feats)\n",
    "            row.update(cadence_feats)\n",
    "            rows.append(row)\n",
    "\n",
    "    data = (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values([\"subject_id\", \"index_time\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    if data.empty:\n",
    "        print(\"No index rows produced. Check your cluster/index settings and data coverage.\")\n",
    "        return\n",
    "\n",
    "    pos = int(data[\"label\"].sum())\n",
    "    neg = int(len(data) - pos)\n",
    "    print(f\"Dataset size (pre-cap): {len(data)}  Positives: {pos}  Negatives: {neg}\")\n",
    "\n",
    "    # Optional: cap negatives per subject (default off)\n",
    "    data = cap_negatives_per_subject(data, args.max_neg_per_subject, seed=42)\n",
    "    pos = int(data[\"label\"].sum())\n",
    "    neg = int(len(data) - pos)\n",
    "    print(f\"Dataset size (post-cap): {len(data)}  Positives: {pos}  Negatives: {neg}\")\n",
    "\n",
    "\n",
    "    meta_cols = [\"subject_id\", \"index_time\", \"index_day\", \"label\", \"source_file\"]\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "\n",
    "    #Missingness indicators are added BEFORE imputation (captures informative missingness)\n",
    "    for c in feature_cols:\n",
    "        data[c + \"_is_missing\"] = data[c].isna().astype(int)\n",
    "\n",
    "    # Refresh feature columns to include missingness flags\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "\n",
    "\n",
    "    files = sorted(data[\"source_file\"].dropna().unique().tolist())\n",
    "    if len(files) < 2:\n",
    "        print(\"Not enough distinct source_file values to run leave-one-CSV-out CV.\")\n",
    "        return\n",
    "\n",
    "    y_true_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    print(f\"\\nLabel horizon: {args.label_start_days}..{args.label_end_days} days\")\n",
    "    print(f\"Negative buffer before cluster starts: {args.negative_buffer_before_cluster_days} days\\n\")\n",
    "\n",
    "    # CV loop: each fold holds out one CSV file\n",
    "    for test_file in files:\n",
    "        test_df = data[data[\"source_file\"] == test_file]\n",
    "        train_df = data[data[\"source_file\"] != test_file]\n",
    "\n",
    "        if test_df.empty or train_df.empty:\n",
    "            continue\n",
    "\n",
    "        X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "        X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_test = test_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "        # Skip fold if training has only one class\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            print(f\"Skipping fold {test_file}: only one class in training.\")\n",
    "            continue\n",
    "\n",
    "        #Impute missing values (fit on TRAIN only to avoid leakage)\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train_imp = imputer.fit_transform(X_train)\n",
    "        X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "        #Scale features (NN training is sensitive to feature scale)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "        X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "        # Oversample minority class (TRAIN only)\n",
    "        X_train_bal, y_train_bal = oversample_minority(X_train_scaled, y_train, random_state=42)\n",
    "\n",
    "        # Fit MLP and predict probabilities\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(args.hidden_units,),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=args.alpha,\n",
    "            learning_rate_init=args.lr,\n",
    "            max_iter=args.max_iter,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.2,\n",
    "            n_iter_no_change=20,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        y_true_all.append(y_test)\n",
    "        y_prob_all.append(y_prob)\n",
    "\n",
    "        # Optional: per-fold reporting (kept brief; pooled results are primary)\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            fold_auroc = roc_auc_score(y_test, y_prob)\n",
    "            fold_auprc = average_precision_score(y_test, y_prob)\n",
    "            print(f\"Fold {test_file}: AUROC={fold_auroc:.3f}  AUPRC={fold_auprc:.3f}  n={len(y_test)}  pos={int(y_test.sum())}\")\n",
    "\n",
    "    if not y_true_all:\n",
    "        print(\"No CV folds produced predictions (check class balance per fold / source_file assignment).\")\n",
    "        return\n",
    "\n",
    "    # Pool out-of-fold predictions\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    y_prob_all = np.concatenate(y_prob_all)\n",
    "\n",
    "\n",
    "    #ONE POOLED EVALUATION\n",
    "\n",
    "\n",
    "    print(\"\\n================== POOLED OOF RESULTS ==================\")\n",
    "    print(f\"Test rows: {len(y_true_all)}  Positives: {int(y_true_all.sum())}  Negatives: {int(len(y_true_all)-y_true_all.sum())}\")\n",
    "    print(f\"Prevalence: {y_true_all.mean():.4f}\")\n",
    "\n",
    "    if len(np.unique(y_true_all)) > 1:\n",
    "        print(\"AUROC:\", round(roc_auc_score(y_true_all, y_prob_all), 3))\n",
    "        print(\"AUPRC:\", round(average_precision_score(y_true_all, y_prob_all), 3))\n",
    "    print(\"Brier:\", round(brier_score_loss(y_true_all, y_prob_all), 4))\n",
    "\n",
    "    # Probability distribution sanity check\n",
    "    qs = np.quantile(y_prob_all, [0.0, 0.5, 0.9, 0.99, 1.0])\n",
    "    print(f\"Predicted prob quantiles [min,50%,90%,99%,max]: {np.round(qs, 4)}\")\n",
    "\n",
    "    # Report @ 0.5 threshold\n",
    "    y_pred_05 = (y_prob_all >= 0.5).astype(int)\n",
    "    print(\"\\n=== Classification report @ threshold = 0.5 ===\")\n",
    "    print(classification_report(y_true_all, y_pred_05, digits=3))\n",
    "\n",
    "    # Best-F1 threshold (pooled PR curve)\n",
    "    thr, best_f1, best_p, best_r = best_f1_threshold(y_true_all, y_prob_all)\n",
    "    y_pred_best = (y_prob_all >= thr).astype(int)\n",
    "    print(\"\\n=== Best-F1 threshold from PR curve ===\")\n",
    "    print(f\"Chosen threshold: {thr:.4f}  (F1={best_f1:.3f}, Precision={best_p:.3f}, Recall={best_r:.3f})\")\n",
    "    print(classification_report(y_true_all, y_pred_best, digits=3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff39c1-31be-41e6-9908-67afa8695e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
