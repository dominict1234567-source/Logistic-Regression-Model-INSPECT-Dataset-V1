{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07e9170-5937-4c72-b40c-c56eb526d4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT IS RUNNING!\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\n",
      "Combined rows: 10502789  Files loaded: 5\n",
      "Unique subjects: 946\n",
      "Dataset size: 62171  Positives: 4058  Negatives: 58113\n",
      "Test rows: 62171  Positives: 4058  Negatives: 58113\n",
      "Prevalence: 0.0653\n",
      "AUROC: 0.641\n",
      "AUPRC: 0.118\n",
      "Brier: 0.2176\n",
      "Predicted prob quantiles [min,50%,90%,99%,max]: [7.000e-04 4.039e-01 6.743e-01 8.738e-01 9.993e-01]\n",
      "\n",
      "=== Classification report @ threshold = 0.5 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.953     0.744     0.836     58113\n",
      "           1      0.116     0.480     0.186      4058\n",
      "\n",
      "    accuracy                          0.727     62171\n",
      "   macro avg      0.535     0.612     0.511     62171\n",
      "weighted avg      0.899     0.727     0.794     62171\n",
      "\n",
      "\n",
      "=== Best-F1 threshold from PR curve ===\n",
      "Chosen threshold: 0.5778  (F1=0.199, Precision=0.136, Recall=0.369)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.950     0.836     0.890     58113\n",
      "           1      0.136     0.369     0.199      4058\n",
      "\n",
      "    accuracy                          0.806     62171\n",
      "   macro avg      0.543     0.603     0.544     62171\n",
      "weighted avg      0.897     0.806     0.844     62171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('IT IS RUNNING!')\n",
    "\"\"\"\n",
    "Neural network version (MLPClassifier: 1 hidden layer, 12 nodes, probabilistic output)\n",
    "\n",
    "Pipeline:\n",
    "1) Load CSVs + tag with source_file\n",
    "2) Build index rows + features + labels\n",
    "3) Leave-one-CSV-out CV\n",
    "4) Impute + scale (NN needs scaling)\n",
    "5) Oversample positives in the TRAIN fold only\n",
    "6) Fit MLPClassifier(12 hidden units), predict_proba, pool metrics\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# DEFAULT SETTINGS\n",
    "DEFAULT_CSV_PATHS = [\"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\"]\n",
    "\n",
    "DEFAULT_TEST_SIZE = 0.2\n",
    "DEFAULT_LOOKBACK_DAYS = 90\n",
    "DEFAULT_SPARSE_GAP_DAYS = 6\n",
    "DEFAULT_DENSE_GAP_DAYS = 2\n",
    "DEFAULT_MIN_CLUSTER_DAYS = 4\n",
    "DEFAULT_CLUSTER_FROM_TABLE = \"measurement\"\n",
    "EXCLUSION_BEFORE_CLUSTER_DAYS = 7\n",
    "PREDICTION_HORIZON_DAYS = 90\n",
    "\n",
    "CODE_SBP = \"LOINC/8480-6\"\n",
    "CODE_HR = \"LOINC/8867-4\"\n",
    "CODE_GLUCOSE = \"SNOMED/271649006\"\n",
    "CODE_HBA1C = \"SNOMED/271650006\"\n",
    "\n",
    "\n",
    "# ---------- helper: oversample minority class in training fold\n",
    "def oversample_minority(X, y, random_state=42):\n",
    "    \"\"\"\n",
    "    Randomly oversample minority class to match majority count.\n",
    "    X: numpy array (n_samples, n_features)\n",
    "    y: numpy array (n_samples,)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    idx_pos = np.where(y == 1)[0]\n",
    "    idx_neg = np.where(y == 0)[0]\n",
    "\n",
    "    if len(idx_pos) == 0 or len(idx_neg) == 0:\n",
    "        return X, y  # can't resample\n",
    "\n",
    "    # Determine minority / majority\n",
    "    if len(idx_pos) < len(idx_neg):\n",
    "        idx_min, idx_maj = idx_pos, idx_neg\n",
    "    else:\n",
    "        idx_min, idx_maj = idx_neg, idx_pos\n",
    "\n",
    "    n_to_add = len(idx_maj) - len(idx_min)\n",
    "    if n_to_add <= 0:\n",
    "        return X, y\n",
    "\n",
    "    extra_idx = rng.choice(idx_min, size=n_to_add, replace=True)\n",
    "    X_bal = np.vstack([X, X[extra_idx]])\n",
    "    y_bal = np.concatenate([y, y[extra_idx]])\n",
    "\n",
    "    # Shuffle\n",
    "    perm = rng.permutation(len(y_bal))\n",
    "    return X_bal[perm], y_bal[perm]\n",
    "\n",
    "\n",
    "# ---------- rest of your functions (unchanged) ----------\n",
    "def parse_mixed_datetime(series):\n",
    "    series = series.astype(\"string\")\n",
    "    mask_slash = series.str.contains(\"/\", na=False)\n",
    "    parsed_slash = pd.to_datetime(series.where(mask_slash), format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
    "    parsed_other = pd.to_datetime(series.where(~mask_slash), errors=\"coerce\", dayfirst=False)\n",
    "    return parsed_slash.fillna(parsed_other)\n",
    "\n",
    "\n",
    "def load_csv_simple(path):\n",
    "    print(\"Loading CSV from\", path)\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\", low_memory=False)\n",
    "    df[\"time\"] = parse_mixed_datetime(df[\"time\"])\n",
    "    df[\"numeric_value\"] = pd.to_numeric(df.get(\"numeric_value\"), errors=\"coerce\")\n",
    "    df = df.sort_values([\"subject_id\", \"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_clusters_for_patient(patient_df, sparse_gap_days, dense_gap_days, min_cluster_days, cluster_from_table):\n",
    "    if cluster_from_table is not None and \"table\" in patient_df.columns:\n",
    "        chosen = patient_df[(patient_df[\"table\"] == cluster_from_table) & (~patient_df[\"time\"].isna())].copy()\n",
    "        if chosen.empty:\n",
    "            chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    else:\n",
    "        chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "\n",
    "    if chosen.empty:\n",
    "        return []\n",
    "\n",
    "    chosen[\"day\"] = chosen[\"time\"].dt.normalize()\n",
    "    unique_days = sorted(chosen[\"day\"].unique().tolist())\n",
    "    clusters = []\n",
    "    if len(unique_days) < min_cluster_days:\n",
    "        return clusters\n",
    "\n",
    "    day_diffs = []\n",
    "    for i in range(1, len(unique_days)):\n",
    "        gap_days = (unique_days[i] - unique_days[i - 1]).days\n",
    "        day_diffs.append(gap_days)\n",
    "\n",
    "    run_start_index = 0\n",
    "    for i, gap in enumerate(day_diffs, start=1):\n",
    "        if gap <= dense_gap_days:\n",
    "            continue\n",
    "        run_end_index = i - 1\n",
    "        run_length = (run_end_index - run_start_index + 1)\n",
    "        if run_length >= min_cluster_days:\n",
    "            preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "            if preceding_gap >= sparse_gap_days:\n",
    "                start_day = unique_days[run_start_index]\n",
    "                end_day = unique_days[run_end_index]\n",
    "                clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "        run_start_index = i\n",
    "\n",
    "    run_end_index = len(unique_days) - 1\n",
    "    run_length = (run_end_index - run_start_index + 1)\n",
    "    if run_length >= min_cluster_days:\n",
    "        preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "        if preceding_gap >= sparse_gap_days:\n",
    "            start_day = unique_days[run_start_index]\n",
    "            end_day = unique_days[run_end_index]\n",
    "            clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def build_index_days(patient_df, clusters, sparse_gap_days):\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        return []\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    cluster_day_set = {\n",
    "        pd.Timestamp(d).normalize()\n",
    "        for start_day, end_day, _ in clusters\n",
    "        for d in pd.date_range(start_day, end_day, freq=\"D\")\n",
    "    }\n",
    "\n",
    "    day_rows = df[[\"day\"]].drop_duplicates().sort_values(\"day\").reset_index(drop=True)\n",
    "    index_list, prev_day = [], None\n",
    "    sid = patient_df[\"subject_id\"].iloc[0]\n",
    "\n",
    "    for i in range(len(day_rows)):\n",
    "        current_day = day_rows.loc[i, \"day\"]\n",
    "        in_cluster = current_day in cluster_day_set\n",
    "        gap_days = math.inf if prev_day is None else (current_day - prev_day).days\n",
    "        if (not in_cluster) and gap_days >= sparse_gap_days:\n",
    "            first_time = df.loc[df[\"day\"] == current_day, \"time\"].min()\n",
    "            if \"source_file\" in df.columns:\n",
    "                src_candidates = df.loc[(df[\"day\"] == current_day) & (df[\"time\"] == first_time), \"source_file\"]\n",
    "                src_file = src_candidates.iloc[0] if len(src_candidates) > 0 else \"unknown\"\n",
    "            else:\n",
    "                src_file = \"unknown\"\n",
    "            index_list.append((sid, first_time, current_day, src_file))\n",
    "        prev_day = current_day\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "def build_features_for_index(patient_df, index_time, lookback_days):\n",
    "    result = {}\n",
    "    meas_df = patient_df.copy()\n",
    "    if \"table\" in meas_df.columns:\n",
    "        meas_df = meas_df[meas_df[\"table\"] == \"measurement\"].copy()\n",
    "\n",
    "    lookback_start = index_time - pd.Timedelta(days=lookback_days)\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "\n",
    "    for code_val in codes:\n",
    "        col_prefix = code_val.replace(\"/\", \"_\")\n",
    "        code_df = meas_df[(meas_df[\"code\"] == code_val) & (~meas_df[\"time\"].isna())].copy()\n",
    "\n",
    "        last_df = code_df[code_df[\"time\"] <= index_time].sort_values(\"time\")\n",
    "        if last_df.empty:\n",
    "            last_val = np.nan\n",
    "            last_days_since = np.nan\n",
    "        else:\n",
    "            last_row = last_df.iloc[-1]\n",
    "            last_val = last_row[\"numeric_value\"]\n",
    "            last_days_since = (index_time - last_row[\"time\"]).days\n",
    "\n",
    "        window_df = code_df[(code_df[\"time\"] >= lookback_start) & (code_df[\"time\"] <= index_time)]\n",
    "        window_vals = window_df[\"numeric_value\"].dropna()\n",
    "\n",
    "        if window_vals.empty:\n",
    "            window_median = np.nan\n",
    "            window_iqr = np.nan\n",
    "            window_count = 0\n",
    "        else:\n",
    "            window_median = float(window_vals.median())\n",
    "            window_count = int(len(window_vals))\n",
    "            if window_count >= 4:\n",
    "                q1 = float(window_vals.quantile(0.25))\n",
    "                q3 = float(window_vals.quantile(0.75))\n",
    "                window_iqr = q3 - q1\n",
    "            else:\n",
    "                window_iqr = np.nan\n",
    "\n",
    "        result[f\"{col_prefix}_last\"] = last_val\n",
    "        result[f\"{col_prefix}_days_since_last\"] = last_days_since\n",
    "        result[f\"{col_prefix}_median_{lookback_days}d\"] = window_median\n",
    "        result[f\"{col_prefix}_iqr_{lookback_days}d\"] = window_iqr\n",
    "        result[f\"{col_prefix}_count_{lookback_days}d\"] = window_count\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def label_index_row(index_time, clusters):\n",
    "    if not clusters:\n",
    "        return 0\n",
    "    horizon_start = (index_time + pd.Timedelta(days=EXCLUSION_BEFORE_CLUSTER_DAYS + 1)).normalize()\n",
    "    horizon_end = (index_time + pd.Timedelta(days=PREDICTION_HORIZON_DAYS)).normalize()\n",
    "    for (start_time, _, _) in clusters:\n",
    "        if (start_time >= horizon_start) and (start_time <= horizon_end):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def best_f1_threshold(y_true, y_prob):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f1 = (2 * precision * recall) / (precision + recall + 1e-12)\n",
    "    best_idx = int(np.nanargmax(f1))\n",
    "    thr = float(thresholds[best_idx]) if best_idx < len(thresholds) else 1.0\n",
    "    return thr, float(f1[best_idx]), float(precision[best_idx]), float(recall[best_idx])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Beginner temporal hospitalization model (multi-CSV)\")\n",
    "    parser.add_argument(\"-f\", default=None, help=argparse.SUPPRESS)\n",
    "    parser.add_argument(\"--csv\", nargs=\"+\", default=DEFAULT_CSV_PATHS, help=\"One or more CSV file paths.\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=DEFAULT_TEST_SIZE)\n",
    "    parser.add_argument(\"--lookback-days\", type=int, default=DEFAULT_LOOKBACK_DAYS)\n",
    "    parser.add_argument(\"--sparse-gap-days\", type=int, default=DEFAULT_SPARSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--dense-gap-days\", type=int, default=DEFAULT_DENSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--min-cluster-days\", type=int, default=DEFAULT_MIN_CLUSTER_DAYS)\n",
    "    parser.add_argument(\"--cluster-from-table\", default=DEFAULT_CLUSTER_FROM_TABLE)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    cluster_from_table = None if str(args.cluster_from_table).lower() == \"none\" else args.cluster_from_table\n",
    "\n",
    "    dfs = []\n",
    "    for path in args.csv:\n",
    "        p = Path(path).expanduser()\n",
    "        part = load_csv_simple(str(p))\n",
    "        part[\"source_file\"] = p.name\n",
    "        dfs.append(part)\n",
    "\n",
    "    df = (pd.concat(dfs, ignore_index=True)\n",
    "          .sort_values([\"subject_id\", \"time\"])\n",
    "          .reset_index(drop=True))\n",
    "    print(f\"Combined rows: {len(df)}  Files loaded: {len(dfs)}\")\n",
    "\n",
    "    subjects = df[\"subject_id\"].dropna().unique()\n",
    "    print(f\"Unique subjects: {len(subjects)}\")\n",
    "\n",
    "    rows = []\n",
    "    for sid, patient in df.groupby(\"subject_id\", sort=False):\n",
    "        clusters = find_clusters_for_patient(\n",
    "            patient,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            dense_gap_days=args.dense_gap_days,\n",
    "            min_cluster_days=args.min_cluster_days,\n",
    "            cluster_from_table=cluster_from_table\n",
    "        )\n",
    "        index_days = build_index_days(patient, clusters, sparse_gap_days=args.sparse_gap_days)\n",
    "\n",
    "        for sid2, idx_time, idx_day, src_file in index_days:\n",
    "            feats = build_features_for_index(patient, idx_time, lookback_days=args.lookback_days)\n",
    "            label = label_index_row(idx_time, clusters)\n",
    "            row = {\n",
    "                \"subject_id\": sid2,\n",
    "                \"index_time\": idx_time,\n",
    "                \"index_day\": idx_day,\n",
    "                \"label\": label,\n",
    "                \"source_file\": src_file\n",
    "            }\n",
    "            row.update(feats)\n",
    "            rows.append(row)\n",
    "\n",
    "    data = (pd.DataFrame(rows)\n",
    "            .sort_values([\"subject_id\", \"index_time\"])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    print(f\"Dataset size: {len(data)}  Positives: {data['label'].sum()}  Negatives: {len(data)-data['label'].sum()}\")\n",
    "\n",
    "    meta_cols = [\"subject_id\", \"index_time\", \"index_day\", \"label\", \"source_file\"]\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "\n",
    "    files = sorted(data[\"source_file\"].dropna().unique().tolist())\n",
    "    if len(files) < 2:\n",
    "        print(\"Not enough distinct source_file values to run leave-one-CSV-out CV.\")\n",
    "        return\n",
    "\n",
    "    y_true_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    for test_file in files:\n",
    "        test_df = data[data[\"source_file\"] == test_file]\n",
    "        train_df = data[data[\"source_file\"] != test_file]\n",
    "\n",
    "        if test_df.empty or train_df.empty:\n",
    "            continue\n",
    "\n",
    "        X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "        X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_test = test_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            continue\n",
    "\n",
    "        # Impute + scale\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        X_train_imp = imputer.fit_transform(X_train)\n",
    "        X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "        X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "        # Oversample minority class in TRAIN fold only (fix imbalance without sample_weight)\n",
    "        X_train_bal, y_train_bal = oversample_minority(X_train_scaled, y_train, random_state=42)\n",
    "\n",
    "        model = MLPClassifier(\n",
    "            hidden_layer_sizes=(12,),\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            alpha=1e-4,\n",
    "            learning_rate_init=1e-3,\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.2,\n",
    "            n_iter_no_change=20,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train_bal, y_train_bal)\n",
    "        y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        y_true_all.append(y_test)\n",
    "        y_prob_all.append(y_prob)\n",
    "\n",
    "    if not y_true_all:\n",
    "        print(\"No CV folds produced predictions (check class balance per fold / source_file assignment).\")\n",
    "        return\n",
    "\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    y_prob_all = np.concatenate(y_prob_all)\n",
    "\n",
    "    print(f\"Test rows: {len(y_true_all)}  Positives: {int(y_true_all.sum())}  Negatives: {int(len(y_true_all)-y_true_all.sum())}\")\n",
    "    print(f\"Prevalence: {y_true_all.mean():.4f}\")\n",
    "\n",
    "    if len(np.unique(y_true_all)) > 1:\n",
    "        print(\"AUROC:\", round(roc_auc_score(y_true_all, y_prob_all), 3))\n",
    "        print(\"AUPRC:\", round(average_precision_score(y_true_all, y_prob_all), 3))\n",
    "\n",
    "    print(\"Brier:\", round(brier_score_loss(y_true_all, y_prob_all), 4))\n",
    "\n",
    "    qs = np.quantile(y_prob_all, [0.0, 0.5, 0.9, 0.99, 1.0])\n",
    "    print(f\"Predicted prob quantiles [min,50%,90%,99%,max]: {np.round(qs, 4)}\")\n",
    "\n",
    "    y_pred_05 = (y_prob_all >= 0.5).astype(int)\n",
    "    print(\"\\n=== Classification report @ threshold = 0.5 ===\")\n",
    "    print(classification_report(y_true_all, y_pred_05, digits=3))\n",
    "\n",
    "    thr, best_f1, best_p, best_r = best_f1_threshold(y_true_all, y_prob_all)\n",
    "    y_pred_best = (y_prob_all >= thr).astype(int)\n",
    "    print(f\"\\n=== Best-F1 threshold from PR curve ===\")\n",
    "    print(f\"Chosen threshold: {thr:.4f}  (F1={best_f1:.3f}, Precision={best_p:.3f}, Recall={best_r:.3f})\")\n",
    "    print(classification_report(y_true_all, y_pred_best, digits=3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0904821-bcb8-424f-851f-92cf8faa941d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
