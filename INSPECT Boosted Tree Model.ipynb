{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b220ce8c-5032-40e4-9a13-794b00c37bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT IS RUNNING!\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\n",
      "Combined rows: 10502789  Files loaded: 5\n",
      "Unique subjects: 946\n",
      "Dataset size: 62171  Positives: 4058  Negatives: 58113\n",
      "Test rows: 62171  Positives: 4058  Negatives: 58113\n",
      "Prevalence: 0.0653\n",
      "AUROC: 0.63\n",
      "AUPRC: 0.118\n",
      "Brier: 0.1939\n",
      "Predicted prob quantiles [min,50%,90%,99%,max]: [0.0028 0.3974 0.6302 0.8022 0.9541]\n",
      "\n",
      "=== Classification report @ threshold = 0.5 ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.951     0.805     0.872     58113\n",
      "           1      0.129     0.412     0.196      4058\n",
      "\n",
      "    accuracy                          0.780     62171\n",
      "   macro avg      0.540     0.609     0.534     62171\n",
      "weighted avg      0.898     0.780     0.828     62171\n",
      "\n",
      "\n",
      "=== Best-F1 threshold from PR curve ===\n",
      "Chosen threshold: 0.5851  (F1=0.201, Precision=0.152, Recall=0.299)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.947     0.883     0.914     58113\n",
      "           1      0.152     0.299     0.201      4058\n",
      "\n",
      "    accuracy                          0.845     62171\n",
      "   macro avg      0.550     0.591     0.558     62171\n",
      "weighted avg      0.896     0.845     0.868     62171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('IT IS RUNNING!')\n",
    "\"\"\"\n",
    "What this script does (UPDATED):\n",
    "1. Load MULTIPLE CSVs with patient events (data_0.csv ... data_4.csv), tag each row with its file of origin (source_file),\n",
    "   and combine into one dataframe. \n",
    "2. For each patient:\n",
    "   - Find clusters of activity days where several readings are taken in conjunction with each other.\n",
    "   - Build index days outside clusters after a sparse gap.\n",
    "   - Compute basic features (last value, days since last, MEDIAN, IQR, count) for 4 codes over lookback window. \n",
    "   - Label: cluster start between days (EXCLUSION_BEFORE_CLUSTER_DAYS+1)..PREDICTION_HORIZON_DAYS after index (default day 8..90).\n",
    "3. Combine all indices into one dataset.\n",
    "4. Train/evaluate Random Forest using 5-fold \"leave-one-CSV-out\" CV (train 5 times),\n",
    "   BUT only print ONE final combined set of metrics using pooled out-of-fold predictions (no per-fold AUROC prints).\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "# Default input CSVs (expects the same schema in each file)\n",
    "# NOTE: These are absolute paths on your machine; change them if you move the project.\n",
    "DEFAULT_CSV_PATHS = [\"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\"]\n",
    "\n",
    "# Kept for compatibility with older single-split versions of the script.\n",
    "# This CV script does NOT use a single global train/test split.  \n",
    "DEFAULT_TEST_SIZE = 0.2  # kept for compatibility (not used as a single split in this CV script) \n",
    "\n",
    "# How far back (in days) we look to compute features for each index timestamp\n",
    "DEFAULT_LOOKBACK_DAYS = 90\n",
    "\n",
    "# Cluster logic:\n",
    "# - sparse_gap_days: minimum quiet gap before a run can be considered a \"new\" cluster\n",
    "# - dense_gap_days: max gap allowed between consecutive observation days inside a cluster\n",
    "# - min_cluster_days: minimum number of activity days required to call it a cluster\n",
    "DEFAULT_SPARSE_GAP_DAYS = 6\n",
    "DEFAULT_DENSE_GAP_DAYS = 2\n",
    "DEFAULT_MIN_CLUSTER_DAYS = 4\n",
    "\n",
    "# Restrict cluster discovery to a specific \"table\" value if present (commonly \"measurement\")\n",
    "DEFAULT_CLUSTER_FROM_TABLE = \"measurement\"\n",
    "\n",
    "# Label window:\n",
    "# Exclude clusters that start too soon after the index day (buffer to avoid leakage / immediate activity)\n",
    "EXCLUSION_BEFORE_CLUSTER_DAYS = 7\n",
    "# How far after index we look for the start of a future cluster (positive label horizon)\n",
    "PREDICTION_HORIZON_DAYS = 90\n",
    "\n",
    "# Clinical codes used as features (4-variable toy set)\n",
    "CODE_SBP = \"LOINC/8480-6\"          # systolic BP\n",
    "CODE_HR = \"LOINC/8867-4\"           # heart rate\n",
    "CODE_GLUCOSE = \"SNOMED/271649006\"  # glucose levels\n",
    "CODE_HBA1C = \"SNOMED/271650006\"    # HbA1c levels\n",
    "\n",
    "# Expected columns (documentation only — script does not hard-enforce this list)\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"subject_id\", \"time\", \"code\", \"numeric_value\", \"care_site_id\", \"clarity_table\",\n",
    "    \"end\", \"note_id\", \"provider_id\", \"table\", \"text_value\", \"unit\", \"visit_id\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Parses mixed-format datetime strings in a pandas Series into datetimes.\n",
    "# - Handles \"dd/mm/YYYY HH:MM\" style with explicit format\n",
    "# - Falls back to pandas parser for other formats\n",
    "def parse_mixed_datetime(series):\n",
    "    series = series.astype(\"string\")\n",
    "    mask_slash = series.str.contains(\"/\", na=False)\n",
    "    parsed_slash = pd.to_datetime(series.where(mask_slash), format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
    "    parsed_other = pd.to_datetime(series.where(~mask_slash), errors=\"coerce\", dayfirst=False)\n",
    "    return parsed_slash.fillna(parsed_other)\n",
    "\n",
    "\n",
    "# Loads a CSV, cleans and parses time and numeric fields, and sorts records by subject and time.\n",
    "# - Parses the \"time\" column (mixed formats supported)\n",
    "# - Coerces numeric_value to float (invalid values become NaN)\n",
    "# - Stable sort by subject_id then time for reproducibility\n",
    "def load_csv_simple(path):\n",
    "    print(\"Loading CSV from\", path)\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\", low_memory=False)\n",
    "    df[\"time\"] = parse_mixed_datetime(df[\"time\"])\n",
    "    df[\"numeric_value\"] = pd.to_numeric(df.get(\"numeric_value\"), errors=\"coerce\")\n",
    "    df = df.sort_values([\"subject_id\", \"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Identifies clusters of consecutive days with observations for a patient based on gap and minimum-length rules.\n",
    "# Cluster definition (day-level):\n",
    "# - collapse events to unique calendar days\n",
    "# - a \"run\" is consecutive activity days where gaps between adjacent days are <= dense_gap_days\n",
    "# - a run becomes a \"cluster\" if:\n",
    "#    (1) run_length >= min_cluster_days\n",
    "#    (2) the run is preceded by a sufficiently sparse gap (>= sparse_gap_days) OR it's the first run\n",
    "# Optional: restrict cluster-finding to a chosen table (e.g., measurement), falling back to all if empty.\n",
    "def find_clusters_for_patient(patient_df, sparse_gap_days, dense_gap_days, min_cluster_days, cluster_from_table):\n",
    "    # Optionally restrict clustering to a specific source table; fall back if empty\n",
    "    if cluster_from_table is not None and \"table\" in patient_df.columns:\n",
    "        chosen = patient_df[(patient_df[\"table\"] == cluster_from_table) & (~patient_df[\"time\"].isna())].copy()\n",
    "        if chosen.empty:\n",
    "            chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    else:\n",
    "        # Use all non-null time observations\n",
    "        chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "\n",
    "    # No valid timestamps → no clusters\n",
    "    if chosen.empty:\n",
    "        return []\n",
    "\n",
    "    # Collapse timestamps to unique calendar days (midnight-normalized)\n",
    "    chosen[\"day\"] = chosen[\"time\"].dt.normalize()\n",
    "    unique_days = sorted(chosen[\"day\"].unique().tolist())\n",
    "\n",
    "    clusters = []\n",
    "\n",
    "    # Not enough days to form a cluster\n",
    "    if len(unique_days) < min_cluster_days:\n",
    "        return clusters\n",
    "\n",
    "    # Compute gaps (in days) between consecutive observation days\n",
    "    day_diffs = []\n",
    "    for i in range(1, len(unique_days)):\n",
    "        gap_days = (unique_days[i] - unique_days[i - 1]).days\n",
    "        day_diffs.append(gap_days)\n",
    "\n",
    "    # Scan for dense runs separated by gaps larger than dense_gap_days\n",
    "    run_start_index = 0\n",
    "    for i, gap in enumerate(day_diffs, start=1):\n",
    "        if gap <= dense_gap_days:\n",
    "            continue\n",
    "\n",
    "        # End current run when gap exceeds dense threshold\n",
    "        run_end_index = i - 1\n",
    "        run_length = (run_end_index - run_start_index + 1)\n",
    "\n",
    "        # Accept run only if long enough and preceded by a sufficiently sparse gap\n",
    "        if run_length >= min_cluster_days:\n",
    "            preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "            if preceding_gap >= sparse_gap_days:\n",
    "                start_day = unique_days[run_start_index]\n",
    "                end_day = unique_days[run_end_index]\n",
    "                clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "        run_start_index = i\n",
    "\n",
    "    # Handle final run after loop\n",
    "    run_end_index = len(unique_days) - 1\n",
    "    run_length = (run_end_index - run_start_index + 1)\n",
    "    if run_length >= min_cluster_days:\n",
    "        preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "        if preceding_gap >= sparse_gap_days:\n",
    "            start_day = unique_days[run_start_index]\n",
    "            end_day = unique_days[run_end_index]\n",
    "            clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "    # Return list of (cluster_start_day, cluster_end_day, number_of_days)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# Builds “index” timestamps for a patient:\n",
    "# - consider unique observed calendar days\n",
    "# - choose days that are NOT in any cluster\n",
    "# - require a sufficiently long quiet gap since the previous observed day (>= sparse_gap_days)\n",
    "# Output rows are day-level indices, but use the earliest timestamp on that day as index_time.\n",
    "#\n",
    "# NEW vs older versions:\n",
    "# - now also returns the source_file for that index day so we can do leave-one-CSV-out CV.  \n",
    "def build_index_days(patient_df, clusters, sparse_gap_days):\n",
    "    # Keep only rows with valid times; if none, no index days\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        return []\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "\n",
    "    # Work at day granularity\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    # Build a set of all calendar days covered by any cluster (inclusive)\n",
    "    cluster_day_set = {\n",
    "        pd.Timestamp(d).normalize()\n",
    "        for start_day, end_day, _ in clusters\n",
    "        for d in pd.date_range(start_day, end_day, freq=\"D\")\n",
    "    }\n",
    "\n",
    "    # Iterate through unique observed days in time order\n",
    "    day_rows = df[[\"day\"]].drop_duplicates().sort_values(\"day\").reset_index(drop=True)\n",
    "\n",
    "    index_list, prev_day = [], None\n",
    "    sid = patient_df[\"subject_id\"].iloc[0]\n",
    "\n",
    "    for i in range(len(day_rows)):\n",
    "        current_day = day_rows.loc[i, \"day\"]\n",
    "        in_cluster = current_day in cluster_day_set\n",
    "        gap_days = math.inf if prev_day is None else (current_day - prev_day).days\n",
    "\n",
    "        # Select \"index\" days: not in any cluster and preceded by a sufficiently long quiet gap\n",
    "        if (not in_cluster) and gap_days >= sparse_gap_days:\n",
    "            # Use the earliest timestamp on that day as the index time\n",
    "            first_time = df.loc[df[\"day\"] == current_day, \"time\"].min()\n",
    "\n",
    "            # Also capture the source file for that (day, first_time) record, if available \n",
    "            # This is what enables leave-one-CSV-out cross-validation at the index-row level.\n",
    "            if \"source_file\" in df.columns:\n",
    "                src_candidates = df.loc[(df[\"day\"] == current_day) & (df[\"time\"] == first_time), \"source_file\"]\n",
    "                src_file = src_candidates.iloc[0] if len(src_candidates) > 0 else \"unknown\"\n",
    "            else:\n",
    "                src_file = \"unknown\"\n",
    "\n",
    "            # Return tuple: (subject_id, index_time, index_day, source_file)  \n",
    "            index_list.append((sid, first_time, current_day, src_file))  \n",
    "        prev_day = current_day\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "# Builds features for ONE (patient, index_time) row using a lookback window.\n",
    "# Features per code:\n",
    "# - last: last observed numeric_value at or before index_time\n",
    "# - days_since_last: days between index_time and that last observation\n",
    "# - median_{lookback_days}d: median of values in the window\n",
    "# - iqr_{lookback_days}d: IQR in the window (computed only when enough points)\n",
    "# - count_{lookback_days}d: number of observed values in the window\n",
    "#\n",
    "# NEW vs older versions:\n",
    "# - now includes median + IQR + count in addition to last/days_since_last. \n",
    "def build_features_for_index(patient_df, index_time, lookback_days):\n",
    "    # Build feature dict for a single index time using measurement data in a lookback window\n",
    "    result = {}\n",
    "    meas_df = patient_df.copy()\n",
    "\n",
    "    # Restrict to measurement table if present (keeps features consistent with cluster_from_table default)\n",
    "    if \"table\" in meas_df.columns:\n",
    "        meas_df = meas_df[meas_df[\"table\"] == \"measurement\"].copy()\n",
    "\n",
    "    lookback_start = index_time - pd.Timedelta(days=lookback_days)\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "\n",
    "    for code_val in codes:\n",
    "        # Prefix for feature names (make code safe for column names)\n",
    "        col_prefix = code_val.replace(\"/\", \"_\")\n",
    "\n",
    "        # Subset to this code with valid timestamps\n",
    "        code_df = meas_df[(meas_df[\"code\"] == code_val) & (~meas_df[\"time\"].isna())].copy()\n",
    "\n",
    "        # \"Last\" value at or before index_time, plus days since it occurred\n",
    "        last_df = code_df[code_df[\"time\"] <= index_time].sort_values(\"time\")\n",
    "        if last_df.empty:\n",
    "            last_val = np.nan\n",
    "            last_days_since = np.nan\n",
    "        else:\n",
    "            last_row = last_df.iloc[-1]\n",
    "            last_val = last_row[\"numeric_value\"]\n",
    "            last_days_since = (index_time - last_row[\"time\"]).days\n",
    "\n",
    "        # Lookback window stats within [lookback_start, index_time]\n",
    "        window_df = code_df[(code_df[\"time\"] >= lookback_start) & (code_df[\"time\"] <= index_time)]\n",
    "        window_vals = window_df[\"numeric_value\"].dropna()\n",
    "\n",
    "        # If no values in window, return NaNs and count=0\n",
    "        if window_vals.empty:\n",
    "            window_median = np.nan\n",
    "            window_iqr = np.nan\n",
    "            window_count = 0\n",
    "        else:\n",
    "            window_median = float(window_vals.median())\n",
    "            window_count = int(len(window_vals))\n",
    "\n",
    "            # IQR stability rule: compute only when there are enough points \n",
    "            # (Avoids noisy/meaningless IQR for tiny sample sizes)\n",
    "            if window_count >= 4:\n",
    "                q1 = float(window_vals.quantile(0.25))\n",
    "                q3 = float(window_vals.quantile(0.75))\n",
    "                window_iqr = q3 - q1\n",
    "            else:\n",
    "                window_iqr = np.nan\n",
    "\n",
    "        # Store features for this code\n",
    "        result[f\"{col_prefix}_last\"] = last_val\n",
    "        result[f\"{col_prefix}_days_since_last\"] = last_days_since\n",
    "        result[f\"{col_prefix}_median_{lookback_days}d\"] = window_median\n",
    "        result[f\"{col_prefix}_iqr_{lookback_days}d\"] = window_iqr\n",
    "        result[f\"{col_prefix}_count_{lookback_days}d\"] = window_count\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Assigns a binary label for an index timestamp:\n",
    "# Positive (1) if ANY cluster starts within the prediction window after index_time.\n",
    "# - Window start = index_time + (EXCLUSION_BEFORE_CLUSTER_DAYS + 1)\n",
    "# - Window end   = index_time + PREDICTION_HORIZON_DAYS\n",
    "# This avoids labeling clusters that begin immediately after the index day (exclusion buffer).\n",
    "def label_index_row(index_time, clusters):\n",
    "    # No clusters means no positive label\n",
    "    if not clusters:\n",
    "        return 0\n",
    "\n",
    "    # Define the prediction window: start after an exclusion buffer, end at the prediction horizon\n",
    "    horizon_start = (index_time + pd.Timedelta(days=EXCLUSION_BEFORE_CLUSTER_DAYS + 1)).normalize()\n",
    "    horizon_end = (index_time + pd.Timedelta(days=PREDICTION_HORIZON_DAYS)).normalize()\n",
    "\n",
    "    # Positive if any cluster starts within [horizon_start, horizon_end]\n",
    "    for (start_time, end_time, n) in clusters:\n",
    "        if (start_time >= horizon_start) and (start_time <= horizon_end):\n",
    "            return 1\n",
    "\n",
    "    # Otherwise negative\n",
    "    return 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    # CLI interface so you can run with different CSVs / parameters without editing the file.\n",
    "    parser = argparse.ArgumentParser(description=\"Beginner temporal hospitalization model (multi-CSV)\")\n",
    "    parser.add_argument(\"-f\", default=None, help=argparse.SUPPRESS)  # keeps compatibility with some notebook runners\n",
    "    parser.add_argument(\"--csv\", nargs=\"+\", default=DEFAULT_CSV_PATHS,\n",
    "                        help=\"One or more CSV file paths.\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=DEFAULT_TEST_SIZE)\n",
    "    parser.add_argument(\"--lookback-days\", type=int, default=DEFAULT_LOOKBACK_DAYS)\n",
    "    parser.add_argument(\"--sparse-gap-days\", type=int, default=DEFAULT_SPARSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--dense-gap-days\", type=int, default=DEFAULT_DENSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--min-cluster-days\", type=int, default=DEFAULT_MIN_CLUSTER_DAYS)\n",
    "    parser.add_argument(\"--cluster-from-table\", default=DEFAULT_CLUSTER_FROM_TABLE)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        # Ignore unknown args rather than failing\n",
    "        print(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    # Allow --cluster-from-table none to disable the restriction\n",
    "    cluster_from_table = None if str(args.cluster_from_table).lower() == \"none\" else args.cluster_from_table\n",
    "\n",
    "    # Load all CSVs, tag each with source_file, then combine  \n",
    "    dfs = []\n",
    "    for path in args.csv:\n",
    "        p = Path(path).expanduser()\n",
    "        part = load_csv_simple(str(p))\n",
    "        # Tag each row with the CSV filename it came from (enables leave-one-CSV-out CV)  \n",
    "        part[\"source_file\"] = p.name\n",
    "        dfs.append(part)\n",
    "\n",
    "    # Combine all patient events across files, keeping chronological order per subject\n",
    "    df = (pd.concat(dfs, ignore_index=True)\n",
    "            .sort_values([\"subject_id\", \"time\"])\n",
    "            .reset_index(drop=True))\n",
    "    print(f\"Combined rows: {len(df)}  Files loaded: {len(dfs)}\")\n",
    "\n",
    "    subjects = df[\"subject_id\"].dropna().unique()\n",
    "    print(f\"Unique subjects: {len(subjects)}\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # Faster + avoids repeated df[df[\"subject_id\"]==sid] scans by grouping once  \n",
    "    for sid, patient in df.groupby(\"subject_id\", sort=False):  \n",
    "        # Find clusters (dense runs of observation days)\n",
    "        clusters = find_clusters_for_patient(\n",
    "            patient,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            dense_gap_days=args.dense_gap_days,\n",
    "            min_cluster_days=args.min_cluster_days,\n",
    "            cluster_from_table=cluster_from_table\n",
    "        )\n",
    "\n",
    "        # Build index days: non-cluster days after sparse gaps\n",
    "        index_days = build_index_days(patient, clusters, sparse_gap_days=args.sparse_gap_days)\n",
    "\n",
    "        # For each index day, compute features + label\n",
    "        for sid2, idx_time, idx_day, src_file in index_days: \n",
    "            feats = build_features_for_index(patient, idx_time, lookback_days=args.lookback_days)\n",
    "            label = label_index_row(idx_time, clusters)\n",
    "            row = {\n",
    "                \"subject_id\": sid2,\n",
    "                \"index_time\": idx_time,\n",
    "                \"index_day\": idx_day,\n",
    "                \"label\": label,\n",
    "                \"source_file\": src_file  \n",
    "            }\n",
    "            row.update(feats)\n",
    "            rows.append(row)\n",
    "\n",
    "    # Final modeling dataset: one row per (patient, index_time)\n",
    "    data = (pd.DataFrame(rows)\n",
    "              .sort_values([\"subject_id\", \"index_time\"])\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    print(f\"Dataset size: {len(data)}  Positives: {data['label'].sum()}  Negatives: {len(data)-data['label'].sum()}\")\n",
    "\n",
    "    # Metadata columns are not used as ML features  \n",
    "    meta_cols = [\"subject_id\", \"index_time\", \"index_day\", \"label\", \"source_file\"]  \n",
    "\n",
    "    # All remaining columns are model features\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "\n",
    "    # Identify distinct CSV sources (each one becomes a held-out fold)  \n",
    "    files = sorted(data[\"source_file\"].dropna().unique().tolist())  \n",
    "    if len(files) < 2:\n",
    "        print(\"Not enough distinct source_file values to run leave-one-CSV-out CV.\")\n",
    "        return\n",
    "\n",
    "    # Accumulate out-of-fold labels and probabilities (so we report ONE combined score) \n",
    "    y_true_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    for test_file in files:\n",
    "        # Split by held-out CSV (test fold = one file, train fold = all other files)  \n",
    "        test_df = data[data[\"source_file\"] == test_file]\n",
    "        train_df = data[data[\"source_file\"] != test_file]\n",
    "\n",
    "        # Skip degenerate folds (should rarely happen, but kept safe)\n",
    "        if test_df.empty or train_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Extract features and labels\n",
    "        X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "        X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_test = test_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "        # Skip fold if training data has only one class (can't fit a classifier)\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            continue\n",
    "\n",
    "        # Trees don't need scaling, but they DO need NaNs handled → median imputation\n",
    "        # Fit imputer on train only to avoid test leakage\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train_imp = imputer.fit_transform(X_train)\n",
    "        X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "        # Train balanced random forest (class_weight helps with imbalance)\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=600,\n",
    "            max_features=\"sqrt\",\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_imp, y_train)\n",
    "\n",
    "        # Predict probabilities for the held-out CSV\n",
    "        y_prob = model.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "        # Store out-of-fold predictions (pooled later)\n",
    "        y_true_all.append(y_test)\n",
    "        y_prob_all.append(y_prob)\n",
    "\n",
    "    # Abort if no valid folds produced predictions\n",
    "    if not y_true_all:\n",
    "        print(\"No CV folds produced predictions (check class balance per fold / source_file assignment).\")\n",
    "        return\n",
    "\n",
    "    # Pool all out-of-fold predictions into one evaluation\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    y_prob_all = np.concatenate(y_prob_all)\n",
    "    y_pred_all = (y_prob_all >= 0.5).astype(int)  # default 0.5 threshold\n",
    "\n",
    "    print(f\"Test rows: {len(y_true_all)}  Positives: {int(y_true_all.sum())}  Negatives: {int(len(y_true_all)-y_true_all.sum())}\")\n",
    "\n",
    "    # Only compute AUROC/AUPRC if both classes exist in pooled test set\n",
    "    if len(np.unique(y_true_all)) > 1:\n",
    "        print(\"AUROC:\", round(roc_auc_score(y_true_all, y_prob_all), 3))\n",
    "        print(\"AUPRC:\", round(average_precision_score(y_true_all, y_prob_all), 3))\n",
    "\n",
    "    # Brier score measures calibration (lower is better)\n",
    "    print(\"Brier:\", round(brier_score_loss(y_true_all, y_prob_all), 4))\n",
    "\n",
    "    # Classification report at threshold 0.5 (precision/recall/F1 per class)\n",
    "    print(classification_report(y_true_all, y_pred_all, digits=3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbf55ea-0dc8-400f-9336-42bb03f4b32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
