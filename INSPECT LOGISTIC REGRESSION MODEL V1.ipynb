{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b805ee1-99ff-4f24-aa9e-948b9f497c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What this script does:\n",
    "1. Load a CSV with patient events.\n",
    "2. For each patient:\n",
    "   - Find clusters of activity days where several reading taken in conjunction with eachother. \n",
    "   - Build index days outside clusters after a sparse gap.\n",
    "   - Compute basic features (last value, days since last, mean, count) for 4 codes over lookback window.\n",
    "   - Label: cluster start between days 8..180 after index.\n",
    "3. Combine all indices, temporal split, train Logistic Regression, evaluate.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# DEFAULT SETTINGS\n",
    "DEFAULT_CSV_PATHS = [\"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\"]\n",
    "\n",
    "DEFAULT_TEST_SIZE = 0.2 #Fraction of rows held out for testing. If 0.2 - means 20% data used for testing. \n",
    "DEFAULT_LOOKBACK_DAYS = 90 #for each index day, look back X days to see what the patient’s measurements were.  \n",
    "DEFAULT_SPARSE_GAP_DAYS = 6 #only mark a day as a new index if there has been at least a X‑day quiet spell \n",
    "DEFAULT_DENSE_GAP_DAYS = 2 #when finding clusters, if days <X days apart, they are part of the same cluster \n",
    "DEFAULT_MIN_CLUSTER_DAYS = 4 #A run of active days must last at least X distinct days to count as a cluster (possible hospitalisation).\n",
    "DEFAULT_CLUSTER_FROM_TABLE = \"measurement\" #dont want to include non-measurements here \n",
    "EXCLUSION_BEFORE_CLUSTER_DAYS = 7 #prevents us including patients already hospitalised - model anticipates, not detects \n",
    "PREDICTION_HORIZON_DAYS = 90 #index = positive if a cluster starts EXCLUSION_BEFORE_CLUSTER_DAYS < cluster <PREDICTION_HORIZON_DAYS\n",
    "\n",
    "CODE_SBP = \"LOINC/8480-6\" #systolic BP\n",
    "CODE_HR = \"LOINC/8867-4\" #heart rate \n",
    "CODE_GLUCOSE = \"SNOMED/271649006\" #glucose levels \n",
    "CODE_HBA1C = \"SNOMED/271650006\" #HBA1c levels \n",
    "\n",
    "EXPECTED_COLUMNS = [  \n",
    "    \"subject_id\",\"time\",\"code\",\"numeric_value\",\"care_site_id\",\"clarity_table\",\n",
    "    \"end\",\"note_id\",\"provider_id\",\"table\",\"text_value\",\"unit\",\"visit_id\"]  #set columns for the INSPECT dataset. \n",
    "\n",
    "#FUNCTIONS\n",
    "\n",
    "#function to ensure consistent datetime values across mixed date formats. \n",
    "def parse_mixed_datetime(series): \n",
    "    series = series.astype(\"string\")\n",
    "    mask_slash = series.str.contains(\"/\", na=False)\n",
    "    parsed_slash = pd.to_datetime(series.where(mask_slash), format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
    "    parsed_other = pd.to_datetime(series.where(~mask_slash), errors=\"coerce\", dayfirst=False)\n",
    "    return parsed_slash.fillna(parsed_other)\n",
    "\n",
    "#check the CSV is in correct format and clean up any messy data. Used AI to ensure ran properly \n",
    "def load_csv_simple(path):\n",
    "    print(\"Loading CSV from\", path)\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\", low_memory=False) #used AI here to encoding=\"utf-8-sig\", low_memory=False\n",
    "    df[\"time\"] = parse_mixed_datetime(df[\"time\"])\n",
    "    df[\"numeric_value\"] = pd.to_numeric(df.get(\"numeric_value\"), errors=\"coerce\")\n",
    "    df = df.sort_values([\"subject_id\",\"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "#used to look for clusters - I used AI to make sure the programme ran correctly i struggling with this. \n",
    "def find_clusters_for_patient(patient_df, sparse_gap_days, dense_gap_days, min_cluster_days, cluster_from_table):\n",
    "    if cluster_from_table is not None and \"table\" in patient_df.columns: #run detection only on measurement rows \n",
    "        chosen = patient_df[(patient_df[\"table\"] == cluster_from_table) & (~patient_df[\"time\"].isna())].copy() #Keeps only relevant activity days and drops rows without timestamps \n",
    "        if chosen.empty: \n",
    "            chosen = patient_df.dropna(subset=[\"time\"]).copy() #no rows of the requested table for this patient, use all timed rows so the patient isn’t skipped entirely.\n",
    "    else:\n",
    "        chosen = patient_df.dropna(subset=[\"time\"]).copy() #Use all rows (with valid time) to detect clusters \n",
    "    if chosen.empty:\n",
    "        return [] #early exit if patient has no time stamps. \n",
    "    chosen[\"day\"] = chosen[\"time\"].dt.normalize() #multiple events on the same calendar day count as a single “active day” in cluster detection.\n",
    "    unique_days = sorted(chosen[\"day\"].unique().tolist()) #Collect sorted unique active days\n",
    "    clusters = [] #storage for clusters \n",
    "    if len(unique_days) < min_cluster_days: #Early exit if not enough days to form even one cluster\n",
    "        return clusters\n",
    "    day_diffs = []\n",
    "    for i in range(1, len(unique_days)):\n",
    "        gap_days = (unique_days[i] - unique_days[i-1]).days\n",
    "        day_diffs.append(gap_days)\n",
    "    run_start_index = 0\n",
    "    for i, gap in enumerate(day_diffs, start=1):\n",
    "        if gap <= dense_gap_days:\n",
    "            continue\n",
    "        run_end_index = i - 1\n",
    "        run_length = (run_end_index - run_start_index + 1)\n",
    "        if run_length >= min_cluster_days:\n",
    "            preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "            if preceding_gap >= sparse_gap_days:\n",
    "                start_day = unique_days[run_start_index]\n",
    "                end_day = unique_days[run_end_index]\n",
    "                clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "        run_start_index = i\n",
    "    run_end_index = len(unique_days) - 1\n",
    "    run_length = (run_end_index - run_start_index + 1)\n",
    "    if run_length >= min_cluster_days:\n",
    "        preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "        if preceding_gap >= sparse_gap_days:\n",
    "            start_day = unique_days[run_start_index]\n",
    "            end_day = unique_days[run_end_index]\n",
    "            clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "    return clusters\n",
    "\n",
    "#“index” timestamps for a patient: looking for days that are outside any cluster and occur after a sufficiently long quiet gap\n",
    "def build_index_days(patient_df, clusters, sparse_gap_days):\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        return []\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    cluster_day_set = {\n",
    "        pd.Timestamp(d).normalize()\n",
    "        for start_day, end_day, _ in clusters\n",
    "        for d in pd.date_range(start_day, end_day, freq=\"D\")}\n",
    "\n",
    "    day_rows = df[[\"day\"]].drop_duplicates().sort_values(\"day\").reset_index(drop=True)\n",
    "    index_list, prev_day = [], None\n",
    "    sid = patient_df[\"subject_id\"].iloc[0]  # keep type as-is\n",
    "\n",
    "    for i in range(len(day_rows)):\n",
    "        current_day = day_rows.loc[i, \"day\"]\n",
    "        in_cluster = current_day in cluster_day_set\n",
    "        gap_days = math.inf if prev_day is None else (current_day - prev_day).days\n",
    "        if (not in_cluster) and gap_days >= sparse_gap_days:\n",
    "            first_time = df.loc[df[\"day\"] == current_day, \"time\"].min()\n",
    "            index_list.append((sid, first_time, current_day))\n",
    "        prev_day = current_day\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "#for each lab (BP/HR/HBA1C/GLUCOSE) finds :\n",
    "#1.The most recent value before the prediction time\n",
    "#2.How long ago that measurement happened\n",
    "#3.The average and count of values in the past N days. \n",
    "#adds to result dict \n",
    "\n",
    "def build_features_for_index(patient_df, index_time, lookback_days):\n",
    "    result = {}\n",
    "    meas_df = patient_df.copy() #producd new df to avoid overwriting\n",
    "    if \"table\" in meas_df.columns:\n",
    "        meas_df = meas_df[meas_df[\"table\"] == \"measurement\"].copy() #filter to measurements - ignore the rest \n",
    "    lookback_start = index_time - pd.Timedelta(days=lookback_days) #calc earliest timestamp included in the lookback window\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "    for code_val in codes:\n",
    "        col_prefix = code_val.replace(\"/\", \"_\") #standardise codes \n",
    "        code_df = meas_df[(meas_df[\"code\"] == code_val) & (~meas_df[\"time\"].isna())].copy() #Filter to rows for the current code with a non-null timestamp\n",
    "        last_df = code_df[code_df[\"time\"] <= index_time].sort_values(\"time\") #Keep only rows occurring at or before the index time (no future leakage) and sort chronologically.\n",
    "        if last_df.empty: # if no last_val/last_days_since, set to None. \n",
    "            last_val = np.nan\n",
    "            last_days_since = np.nan\n",
    "        else:             \n",
    "            last_row = last_df.iloc[-1] #take last row after sort and \n",
    "            last_val = last_row[\"numeric_value\"] #store its value \n",
    "            last_days_since = (index_time - last_row[\"time\"]).days #Compute how many full days ago it occurred relative to index_time\n",
    "        window_df = code_df[(code_df[\"time\"] >= lookback_start) & (code_df[\"time\"] <= index_time)]\n",
    "        window_vals = window_df[\"numeric_value\"].dropna()\n",
    "        if window_vals.empty:\n",
    "            window_mean = np.nan\n",
    "            window_count = 0\n",
    "        else:\n",
    "            window_mean = float(window_vals.mean())\n",
    "            window_count = int(len(window_vals))\n",
    "        result[f\"{col_prefix}_last\"] = last_val\n",
    "        result[f\"{col_prefix}_days_since_last\"] = last_days_since\n",
    "        result[f\"{col_prefix}_mean_{lookback_days}d\"] = window_mean\n",
    "        result[f\"{col_prefix}_count_{lookback_days}d\"] = window_count\n",
    "    return result\n",
    "\n",
    "\n",
    "#Will a cluster start between horizon_start and horizon_end?”\n",
    "#1 is pos (cluster starts)\n",
    "#0 is neg (cluster doesnt start)\n",
    "def label_index_row(index_time, clusters): \n",
    "    if not clusters: \n",
    "        return 0\n",
    "    horizon_start = (index_time + pd.Timedelta(days=EXCLUSION_BEFORE_CLUSTER_DAYS + 1)).normalize()\n",
    "    horizon_end = (index_time + pd.Timedelta(days=PREDICTION_HORIZON_DAYS)).normalize()\n",
    "    for (start_time,end_time,n) in clusters:\n",
    "        if (start_time >= horizon_start) and (start_time <= horizon_end):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "'''USED AI FOR THIS:::\n",
    "Split a dataset of index rows into a training and testing set based on chronological order (not random). \n",
    "The most recent fraction (defined by test_size) becomes the test set; earlier rows become the training set. \n",
    "It returns two NumPy arrays of row indices: train_idx and test_idx. \n",
    "'''\n",
    "def temporal_split(dataset_df, test_size):\n",
    "    df_sorted = dataset_df.sort_values(\"index_time\").reset_index(drop=True)\n",
    "    if df_sorted.empty:\n",
    "        return np.array([],dtype=int), np.array([],dtype=int)\n",
    "    cutoff = df_sorted[\"index_time\"].quantile(1 - test_size)\n",
    "    train_idx = df_sorted.index[df_sorted[\"index_time\"] < cutoff].to_numpy()\n",
    "    test_idx = df_sorted.index[df_sorted[\"index_time\"] >= cutoff].to_numpy()\n",
    "    if len(train_idx) == 0 and len(df_sorted) >= 2:\n",
    "        train_idx = np.arange(0, len(df_sorted)-1)\n",
    "        test_idx = np.arange(len(df_sorted)-1, len(df_sorted))\n",
    "    if len(test_idx) == 0 and len(df_sorted) >= 2:\n",
    "        split_pt = int(np.floor((1 - test_size)*len(df_sorted)))\n",
    "        train_idx = np.arange(0, max(split_pt,1))\n",
    "        test_idx = np.arange(max(split_pt,1), len(df_sorted))\n",
    "    return train_idx, test_idx\n",
    "\n",
    "\n",
    "#loading data in using argparse. \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Beginner temporal hospitalization model (multi-CSV)\")\n",
    "    parser.add_argument(\"-f\", default=None, help=argparse.SUPPRESS)\n",
    "    parser.add_argument(\"--csv\", nargs=\"+\", default=DEFAULT_CSV_PATHS,\n",
    "                        help=\"One or more CSV file paths.\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=DEFAULT_TEST_SIZE)\n",
    "    parser.add_argument(\"--lookback-days\", type=int, default=DEFAULT_LOOKBACK_DAYS)\n",
    "    parser.add_argument(\"--sparse-gap-days\", type=int, default=DEFAULT_SPARSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--dense-gap-days\", type=int, default=DEFAULT_DENSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--min-cluster-days\", type=int, default=DEFAULT_MIN_CLUSTER_DAYS)\n",
    "    parser.add_argument(\"--cluster-from-table\", default=DEFAULT_CLUSTER_FROM_TABLE)\n",
    "    parser.add_argument(\"--out-features\", default=None)\n",
    "    parser.add_argument(\"--out-preds\", default=None)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    cluster_from_table = None if str(args.cluster_from_table).lower() == \"none\" else args.cluster_from_table\n",
    "    \n",
    "#combine the 5 CSVs into a single CSV\n",
    "    dfs = []\n",
    "    for path in args.csv:\n",
    "        p = Path(path).expanduser()\n",
    "        part = load_csv_simple(str(p))\n",
    "        dfs.append(part)\n",
    "    \n",
    "    df = (pd.concat(dfs, ignore_index=True)\n",
    "            .sort_values([\"subject_id\", \"time\"])\n",
    "            .reset_index(drop=True))\n",
    "    print(f\"Combined rows: {len(df)}  Files loaded: {len(dfs)}\")\n",
    "    \n",
    "    subjects = df[\"subject_id\"].dropna().unique()\n",
    "    print(f\"Unique subjects: {len(subjects)}\") \n",
    "    \n",
    "    rows = []\n",
    "    for sid in subjects: #make a dffor each unique patient \n",
    "        patient = df[df[\"subject_id\"] == sid]\n",
    "        clusters = find_clusters_for_patient( #use previous cluster function to find patient clusters \n",
    "            patient,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            dense_gap_days=args.dense_gap_days,\n",
    "            min_cluster_days=args.min_cluster_days,\n",
    "            cluster_from_table=cluster_from_table)\n",
    "        index_days = build_index_days(patient, clusters, sparse_gap_days=args.sparse_gap_days)#use previous function to look for days that are outside any cluster\n",
    "        for sid2, idx_time, idx_day in index_days: #for patients index days \n",
    "            feats = build_features_for_index(patient, idx_time, lookback_days=args.lookback_days) #use previous function to find code values in and out of cluster \n",
    "            label = label_index_row(idx_time, clusters) #looks to see whether a cluster starts or doesnt start\n",
    "            row = {\"subject_id\": sid2, \"index_time\": idx_time, \"index_day\": idx_day, \"label\": label}\n",
    "            row.update(feats) #Merges the feature dictionary into the row dictionary\n",
    "            rows.append(row)\n",
    "\n",
    "    data = (pd.DataFrame(rows) #Converts the accumulated list of dicts above into a pandas DataFrame\n",
    "              .sort_values([\"subject_id\", \"index_time\"])\n",
    "              .reset_index(drop=True))\n",
    "    print(f\"Dataset size: {len(data)}  Positives: {data['label'].sum()}  Negatives: {len(data)-data['label'].sum()}\")\n",
    "\n",
    "    \n",
    "    train_idx, test_idx = temporal_split(data, args.test_size) #split the dataset (data we made above) into training and test indices and set parameters for training \n",
    "    meta_cols = [\"subject_id\", \"index_time\", \"index_day\", \"label\"]\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "    X = data[feature_cols].to_numpy(dtype=float) #extract features into numpy \n",
    "    y = data[\"label\"].astype(int).to_numpy() #target labels (1 for cluster, 0 for not) into numpy \n",
    "    X_train, y_train = X[train_idx], y[train_idx] #feature and label selected training\n",
    "    X_test, y_test = X[test_idx], y[test_idx]#feature and label selected testing\n",
    "\n",
    "    if len(np.unique(y_train)) < 2: #make sure enough features to train \n",
    "        print(\"One class in training set.\")\n",
    "        return\n",
    "\n",
    "    #missing training and test data training medians\n",
    "    imputer = SimpleImputer(strategy=\"median\") \n",
    "    scaler = StandardScaler() \n",
    "    X_train_imp = imputer.fit_transform(X_train)\n",
    "    X_test_imp = imputer.transform(X_test)\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imp)\n",
    "    X_test_scaled = scaler.transform(X_test_imp)\n",
    "\n",
    "    #model this \n",
    "    model = LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    #print our values out. \n",
    "    print(f\"Test rows: {len(y_test)}  Positives: {y_test.sum()}  Negatives: {len(y_test)-y_test.sum()}\")\n",
    "    if len(np.unique(y_test)) > 1:\n",
    "        try: print(\"AUROC:\", round(roc_auc_score(y_test, y_prob), 3))\n",
    "        except: pass\n",
    "        try: print(\"AUPRC:\", round(average_precision_score(y_test, y_prob), 3))\n",
    "        except: pass\n",
    "        try: print(\"Brier:\", round(brier_score_loss(y_test, y_prob), 4))\n",
    "        except: pass\n",
    "    print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e7fffc-231f-4c53-88e2-5f7eda96e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\n",
      "Combined rows: 10502789  Files loaded: 5\n",
      "Unique subjects: 946\n",
      "Dataset size: 62171  Positives: 4058  Negatives: 58113\n",
      "Test rows: 12441  Positives: 739  Negatives: 11702\n",
      "AUROC: 0.632\n",
      "AUPRC: 0.127\n",
      "Brier: 0.2315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.957     0.731     0.829     11702\n",
      "           1      0.102     0.486     0.169       739\n",
      "\n",
      "    accuracy                          0.716     12441\n",
      "   macro avg      0.530     0.608     0.499     12441\n",
      "weighted avg      0.907     0.716     0.790     12441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9f4e0-b376-4069-bc57-e8fec25004d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
