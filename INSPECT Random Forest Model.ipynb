{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e98f7ee-030e-4369-a9c4-200071ea33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT IS RUNNING!\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\n",
      "Loading CSV from /Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\n",
      "Combined rows: 10502789  Files loaded: 5\n",
      "Unique subjects: 946\n",
      "Dataset size: 62171  Positives: 4058  Negatives: 58113\n",
      "Test rows: 62171  Positives: 4058  Negatives: 58113\n",
      "AUROC: 0.562\n",
      "AUPRC: 0.078\n",
      "Brier: 0.0999\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.935     0.995     0.964     58113\n",
      "           1      0.181     0.016     0.030      4058\n",
      "\n",
      "    accuracy                          0.931     62171\n",
      "   macro avg      0.558     0.506     0.497     62171\n",
      "weighted avg      0.886     0.931     0.903     62171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('IT IS RUNNING!')\n",
    "\"\"\"\n",
    "What this script does (UPDATED):\n",
    "1. Load MULTIPLE CSVs with patient events (data_0.csv ... data_4.csv), tag each row with its file of origin (source_file),\n",
    "   and combine into one dataframe.  #CHANGES MADE#\n",
    "2. For each patient:\n",
    "   - Find clusters of activity days where several readings are taken in conjunction with each other.\n",
    "   - Build index days outside clusters after a sparse gap.\n",
    "   - Compute basic features (last value, days since last, MEDIAN, IQR, count) for 4 codes over lookback window.  #CHANGES MADE#\n",
    "   - Label: cluster start between days (EXCLUSION_BEFORE_CLUSTER_DAYS+1)..PREDICTION_HORIZON_DAYS after index (default day 8..90).\n",
    "3. Combine all indices into one dataset.\n",
    "4. Train/evaluate Random Forest using 5-fold \"leave-one-CSV-out\" CV (train 5 times),\n",
    "   BUT only print ONE final combined set of metrics using pooled out-of-fold predictions (no per-fold AUROC prints).  #CHANGES MADE#\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# DEFAULT SETTINGS\n",
    "DEFAULT_CSV_PATHS = [\"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_0.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_1.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_2.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_3.csv\",\n",
    "                     \"/Users/colleenohare/Desktop/Bioinformatics_MSC/RETFOUND/Chris_Sainsbury/Inspect_Dataset/inspect_data_csv/data_4.csv\"]\n",
    "\n",
    "DEFAULT_TEST_SIZE = 0.2  # kept for compatibility (not used as a single split in this CV script)  #CHANGES MADE#\n",
    "DEFAULT_LOOKBACK_DAYS = 90\n",
    "DEFAULT_SPARSE_GAP_DAYS = 6\n",
    "DEFAULT_DENSE_GAP_DAYS = 2\n",
    "DEFAULT_MIN_CLUSTER_DAYS = 4\n",
    "DEFAULT_CLUSTER_FROM_TABLE = \"measurement\"\n",
    "EXCLUSION_BEFORE_CLUSTER_DAYS = 7\n",
    "PREDICTION_HORIZON_DAYS = 90\n",
    "\n",
    "CODE_SBP = \"LOINC/8480-6\"  # systolic BP\n",
    "CODE_HR = \"LOINC/8867-4\"  # heart rate\n",
    "CODE_GLUCOSE = \"SNOMED/271649006\"  # glucose levels\n",
    "CODE_HBA1C = \"SNOMED/271650006\"  # HBA1c levels\n",
    "\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"subject_id\", \"time\", \"code\", \"numeric_value\", \"care_site_id\", \"clarity_table\",\n",
    "    \"end\", \"note_id\", \"provider_id\", \"table\", \"text_value\", \"unit\", \"visit_id\"\n",
    "]\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "\n",
    "#Parses mixed-format datetime strings in a pandas Series into datetimes.\n",
    "def parse_mixed_datetime(series):\n",
    "    series = series.astype(\"string\")\n",
    "    mask_slash = series.str.contains(\"/\", na=False)\n",
    "    parsed_slash = pd.to_datetime(series.where(mask_slash), format=\"%d/%m/%Y %H:%M\", errors=\"coerce\")\n",
    "    parsed_other = pd.to_datetime(series.where(~mask_slash), errors=\"coerce\", dayfirst=False)\n",
    "    return parsed_slash.fillna(parsed_other)\n",
    "\n",
    "\n",
    "#Loads a CSV, cleans and parses time and numeric fields, and sorts records by subject and time.\n",
    "def load_csv_simple(path):\n",
    "    print(\"Loading CSV from\", path)\n",
    "    df = pd.read_csv(path, sep=\",\", encoding=\"utf-8-sig\", low_memory=False)\n",
    "    df[\"time\"] = parse_mixed_datetime(df[\"time\"])\n",
    "    df[\"numeric_value\"] = pd.to_numeric(df.get(\"numeric_value\"), errors=\"coerce\")\n",
    "    df = df.sort_values([\"subject_id\", \"time\"], kind=\"mergesort\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Identifies clusters of consecutive days with observations for a patient based on gap and minimum-length rules.\n",
    "def find_clusters_for_patient(patient_df, sparse_gap_days, dense_gap_days, min_cluster_days, cluster_from_table):\n",
    "    # Optionally restrict clustering to a specific source table; fall back if empty\n",
    "    if cluster_from_table is not None and \"table\" in patient_df.columns:\n",
    "        chosen = patient_df[(patient_df[\"table\"] == cluster_from_table) & (~patient_df[\"time\"].isna())].copy()\n",
    "        if chosen.empty:\n",
    "            chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    else:\n",
    "        # Use all non-null time observations\n",
    "        chosen = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "\n",
    "    # No valid timestamps → no clusters\n",
    "    if chosen.empty:\n",
    "        return []\n",
    "\n",
    "    # Collapse timestamps to unique calendar days\n",
    "    chosen[\"day\"] = chosen[\"time\"].dt.normalize()\n",
    "    unique_days = sorted(chosen[\"day\"].unique().tolist())\n",
    "\n",
    "    clusters = []\n",
    "\n",
    "    # Not enough days to form a cluster\n",
    "    if len(unique_days) < min_cluster_days:\n",
    "        return clusters\n",
    "\n",
    "    # Compute gaps (in days) between consecutive observation days\n",
    "    day_diffs = []\n",
    "    for i in range(1, len(unique_days)):\n",
    "        gap_days = (unique_days[i] - unique_days[i - 1]).days\n",
    "        day_diffs.append(gap_days)\n",
    "\n",
    "    # Scan for dense runs separated by gaps larger than dense_gap_days\n",
    "    run_start_index = 0\n",
    "    for i, gap in enumerate(day_diffs, start=1):\n",
    "        if gap <= dense_gap_days:\n",
    "            continue\n",
    "\n",
    "        # End current run when gap exceeds dense threshold\n",
    "        run_end_index = i - 1\n",
    "        run_length = (run_end_index - run_start_index + 1)\n",
    "\n",
    "        # Accept run only if long enough and preceded by a sufficiently sparse gap\n",
    "        if run_length >= min_cluster_days:\n",
    "            preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "            if preceding_gap >= sparse_gap_days:\n",
    "                start_day = unique_days[run_start_index]\n",
    "                end_day = unique_days[run_end_index]\n",
    "                clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "        run_start_index = i\n",
    "\n",
    "    # Handle final run after loop\n",
    "    run_end_index = len(unique_days) - 1\n",
    "    run_length = (run_end_index - run_start_index + 1)\n",
    "    if run_length >= min_cluster_days:\n",
    "        preceding_gap = math.inf if run_start_index == 0 else day_diffs[run_start_index - 1]\n",
    "        if preceding_gap >= sparse_gap_days:\n",
    "            start_day = unique_days[run_start_index]\n",
    "            end_day = unique_days[run_end_index]\n",
    "            clusters.append((start_day.normalize(), end_day.normalize(), run_length))\n",
    "\n",
    "    # Return list of (cluster_start_day, cluster_end_day, number_of_days)\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# “index” timestamps for a patient: days outside any cluster after a sufficiently long quiet gap\n",
    "# Now also returns the source_file for that index day (so we can do leave-one-CSV-out CV).  #CHANGES MADE#\n",
    "def build_index_days(patient_df, clusters, sparse_gap_days):\n",
    "    # Keep only rows with valid times; if none, no index days\n",
    "    df = patient_df.dropna(subset=[\"time\"]).copy()\n",
    "    if df.empty:\n",
    "        return []\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"time\"])\n",
    "\n",
    "    # Work at day granularity\n",
    "    df[\"day\"] = df[\"time\"].dt.normalize()\n",
    "\n",
    "    # Build a set of all calendar days covered by any cluster (inclusive)\n",
    "    cluster_day_set = {\n",
    "        pd.Timestamp(d).normalize()\n",
    "        for start_day, end_day, _ in clusters\n",
    "        for d in pd.date_range(start_day, end_day, freq=\"D\")\n",
    "    }\n",
    "\n",
    "    # Iterate through unique observed days in time order\n",
    "    day_rows = df[[\"day\"]].drop_duplicates().sort_values(\"day\").reset_index(drop=True)\n",
    "\n",
    "    index_list, prev_day = [], None\n",
    "    sid = patient_df[\"subject_id\"].iloc[0]\n",
    "\n",
    "    for i in range(len(day_rows)):\n",
    "        current_day = day_rows.loc[i, \"day\"]\n",
    "        in_cluster = current_day in cluster_day_set\n",
    "        gap_days = math.inf if prev_day is None else (current_day - prev_day).days\n",
    "\n",
    "        # Select \"index\" days: not in any cluster and preceded by a sufficiently long quiet gap\n",
    "        if (not in_cluster) and gap_days >= sparse_gap_days:\n",
    "            # Use the earliest timestamp on that day as the index time\n",
    "            first_time = df.loc[df[\"day\"] == current_day, \"time\"].min()\n",
    "            # get the file tag for the row at (current_day, first_time)  #CHANGES MADE#\n",
    "            # Also capture the source file for that (day, first_time) record, if available\n",
    "            if \"source_file\" in df.columns:\n",
    "                src_candidates = df.loc[(df[\"day\"] == current_day) & (df[\"time\"] == first_time), \"source_file\"]\n",
    "                src_file = src_candidates.iloc[0] if len(src_candidates) > 0 else \"unknown\"\n",
    "            else:\n",
    "                src_file = \"unknown\"\n",
    "            # Return tuple: (subject_id, index_time, index_day, source_file)\n",
    "            index_list.append((sid, first_time, current_day, src_file))  #CHANGES MADE#\n",
    "        prev_day = current_day\n",
    "\n",
    "    return index_list\n",
    "\n",
    "\n",
    "# Features: last, days_since_last, median, IQR, count  #CHANGES MADE#\n",
    "def build_features_for_index(patient_df, index_time, lookback_days):\n",
    "    # Build feature dict for a single index time using measurement data in a lookback window\n",
    "    result = {}\n",
    "    meas_df = patient_df.copy()\n",
    "\n",
    "    # Restrict to measurement table if present\n",
    "    if \"table\" in meas_df.columns:\n",
    "        meas_df = meas_df[meas_df[\"table\"] == \"measurement\"].copy()\n",
    "\n",
    "    lookback_start = index_time - pd.Timedelta(days=lookback_days)\n",
    "    codes = [CODE_SBP, CODE_HR, CODE_GLUCOSE, CODE_HBA1C]\n",
    "\n",
    "    for code_val in codes:\n",
    "        # Prefix for feature names (make code safe for column names)\n",
    "        col_prefix = code_val.replace(\"/\", \"_\")\n",
    "\n",
    "        # Subset to this code with valid timestamps\n",
    "        code_df = meas_df[(meas_df[\"code\"] == code_val) & (~meas_df[\"time\"].isna())].copy()\n",
    "\n",
    "        # \"Last\" value at or before index_time, plus days since it occurred\n",
    "        last_df = code_df[code_df[\"time\"] <= index_time].sort_values(\"time\")\n",
    "        if last_df.empty:\n",
    "            last_val = np.nan\n",
    "            last_days_since = np.nan\n",
    "        else:\n",
    "            last_row = last_df.iloc[-1]\n",
    "            last_val = last_row[\"numeric_value\"]\n",
    "            last_days_since = (index_time - last_row[\"time\"]).days\n",
    "\n",
    "        # Lookback window stats within [lookback_start, index_time]\n",
    "        window_df = code_df[(code_df[\"time\"] >= lookback_start) & (code_df[\"time\"] <= index_time)]\n",
    "        window_vals = window_df[\"numeric_value\"].dropna()\n",
    "\n",
    "        # If no values in window, return NaNs and count=0\n",
    "        if window_vals.empty:\n",
    "            window_median = np.nan\n",
    "            window_iqr = np.nan\n",
    "            window_count = 0\n",
    "        else:\n",
    "            window_median = float(window_vals.median())\n",
    "            window_count = int(len(window_vals))\n",
    "            # IQR stability rule: compute only when there are enough points  #CHANGES MADE#\n",
    "            # IQR is only computed when there are enough points to be meaningful\n",
    "            if window_count >= 4:\n",
    "                q1 = float(window_vals.quantile(0.25))\n",
    "                q3 = float(window_vals.quantile(0.75))\n",
    "                window_iqr = q3 - q1\n",
    "            else:\n",
    "                window_iqr = np.nan\n",
    "\n",
    "        # Store features for this code\n",
    "        result[f\"{col_prefix}_last\"] = last_val\n",
    "        result[f\"{col_prefix}_days_since_last\"] = last_days_since\n",
    "        result[f\"{col_prefix}_median_{lookback_days}d\"] = window_median\n",
    "        result[f\"{col_prefix}_iqr_{lookback_days}d\"] = window_iqr\n",
    "        result[f\"{col_prefix}_count_{lookback_days}d\"] = window_count\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "#Assign a binary label indicating whether a future cluster begins within the prediction window after the index time.\n",
    "def label_index_row(index_time, clusters):\n",
    "    # No clusters means no positive label\n",
    "    if not clusters:\n",
    "        return 0\n",
    "    # Define the prediction window: start after an exclusion buffer, end at the prediction horizon\n",
    "    horizon_start = (index_time + pd.Timedelta(days=EXCLUSION_BEFORE_CLUSTER_DAYS + 1)).normalize()\n",
    "    horizon_end = (index_time + pd.Timedelta(days=PREDICTION_HORIZON_DAYS)).normalize()\n",
    "    # Positive if any cluster starts within [horizon_start, horizon_end]\n",
    "    for (start_time, end_time, n) in clusters:\n",
    "        if (start_time >= horizon_start) and (start_time <= horizon_end):\n",
    "            return 1\n",
    "    # Otherwise negative\n",
    "    return 0\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Beginner temporal hospitalization model (multi-CSV)\")\n",
    "    parser.add_argument(\"-f\", default=None, help=argparse.SUPPRESS)\n",
    "    parser.add_argument(\"--csv\", nargs=\"+\", default=DEFAULT_CSV_PATHS,\n",
    "                        help=\"One or more CSV file paths.\")\n",
    "    parser.add_argument(\"--test-size\", type=float, default=DEFAULT_TEST_SIZE)\n",
    "    parser.add_argument(\"--lookback-days\", type=int, default=DEFAULT_LOOKBACK_DAYS)\n",
    "    parser.add_argument(\"--sparse-gap-days\", type=int, default=DEFAULT_SPARSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--dense-gap-days\", type=int, default=DEFAULT_DENSE_GAP_DAYS)\n",
    "    parser.add_argument(\"--min-cluster-days\", type=int, default=DEFAULT_MIN_CLUSTER_DAYS)\n",
    "    parser.add_argument(\"--cluster-from-table\", default=DEFAULT_CLUSTER_FROM_TABLE)\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"Ignoring unknown args: {unknown}\")\n",
    "\n",
    "    cluster_from_table = None if str(args.cluster_from_table).lower() == \"none\" else args.cluster_from_table\n",
    "\n",
    "    # Load all CSVs, tag each with source_file, then combine  #CHANGES MADE#\n",
    "    dfs = []\n",
    "    for path in args.csv:\n",
    "        p = Path(path).expanduser()\n",
    "        part = load_csv_simple(str(p))\n",
    "        part[\"source_file\"] = p.name  #CHANGES MADE#\n",
    "        dfs.append(part)\n",
    "\n",
    "    df = (pd.concat(dfs, ignore_index=True)\n",
    "            .sort_values([\"subject_id\", \"time\"])\n",
    "            .reset_index(drop=True))\n",
    "    print(f\"Combined rows: {len(df)}  Files loaded: {len(dfs)}\")\n",
    "\n",
    "    subjects = df[\"subject_id\"].dropna().unique()\n",
    "    print(f\"Unique subjects: {len(subjects)}\")\n",
    "\n",
    "    rows = []\n",
    "    # Faster + avoids repeated df[df[\"subject_id\"]==sid] scans  #CHANGES MADE#\n",
    "    # Faster + avoids repeated df[df[\"subject_id\"]==sid] scans\n",
    "    for sid, patient in df.groupby(\"subject_id\", sort=False):  #CHANGES MADE#\n",
    "        clusters = find_clusters_for_patient(\n",
    "            patient,\n",
    "            sparse_gap_days=args.sparse_gap_days,\n",
    "            dense_gap_days=args.dense_gap_days,\n",
    "            min_cluster_days=args.min_cluster_days,\n",
    "            cluster_from_table=cluster_from_table\n",
    "        )\n",
    "        index_days = build_index_days(patient, clusters, sparse_gap_days=args.sparse_gap_days)\n",
    "\n",
    "        for sid2, idx_time, idx_day, src_file in index_days:  #CHANGES MADE#\n",
    "            feats = build_features_for_index(patient, idx_time, lookback_days=args.lookback_days)\n",
    "            label = label_index_row(idx_time, clusters)\n",
    "            row = {\n",
    "                \"subject_id\": sid2,\n",
    "                \"index_time\": idx_time,\n",
    "                \"index_day\": idx_day,\n",
    "                \"label\": label,\n",
    "                \"source_file\": src_file  #CHANGES MADE#\n",
    "            }\n",
    "            row.update(feats)\n",
    "            rows.append(row)\n",
    "\n",
    "    data = (pd.DataFrame(rows)\n",
    "              .sort_values([\"subject_id\", \"index_time\"])\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "    print(f\"Dataset size: {len(data)}  Positives: {data['label'].sum()}  Negatives: {len(data)-data['label'].sum()}\")\n",
    "\n",
    "    # Leave-one-CSV-out CV: train 5 times, pool out-of-fold predictions, print ONE final report  #CHANGES MADE#\n",
    "    meta_cols = [\"subject_id\", \"index_time\", \"index_day\", \"label\", \"source_file\"]  #CHANGES MADE#\n",
    "\n",
    "    # All remaining columns are model features\n",
    "    feature_cols = [c for c in data.columns if c not in meta_cols]\n",
    "\n",
    "    files = sorted(data[\"source_file\"].dropna().unique().tolist())  #CHANGES MADE#\n",
    "    # Identify distinct CSV sources (used as CV folds)\n",
    "    if len(files) < 2:\n",
    "        print(\"Not enough distinct source_file values to run leave-one-CSV-out CV.\")\n",
    "        return\n",
    "\n",
    "    # Accumulate out-of-fold labels and probabilities\n",
    "    y_true_all = []\n",
    "    y_prob_all = []\n",
    "\n",
    "    for test_file in files:\n",
    "        # Split by held-out CSV\n",
    "        test_df = data[data[\"source_file\"] == test_file]\n",
    "        train_df = data[data[\"source_file\"] != test_file]\n",
    "\n",
    "        # Skip degenerate folds\n",
    "        if test_df.empty or train_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Extract features and labels\n",
    "        X_train = train_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_train = train_df[\"label\"].astype(int).to_numpy()\n",
    "        X_test = test_df[feature_cols].to_numpy(dtype=float)\n",
    "        y_test = test_df[\"label\"].astype(int).to_numpy()\n",
    "\n",
    "        # if training fold has one class, skip fold\n",
    "        # Skip fold if training data has only one class\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            continue\n",
    "\n",
    "        # Trees don't need scaling, but they do need NaNs handled\n",
    "        # Impute missing values and scale features (fit on train only)\n",
    "        imputer = SimpleImputer(strategy=\"median\")\n",
    "        X_train_imp = imputer.fit_transform(X_train)\n",
    "        X_test_imp = imputer.transform(X_test)\n",
    "\n",
    "        # Train balanced random forest\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=600,\n",
    "            max_features=\"sqrt\",\n",
    "            min_samples_leaf=5,\n",
    "            class_weight=\"balanced_subsample\",\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train_imp, y_train)\n",
    "\n",
    "        # Predict probabilities for the held-out CSV\n",
    "        y_prob = model.predict_proba(X_test_imp)[:, 1]\n",
    "\n",
    "        # Store out-of-fold predictions\n",
    "        y_true_all.append(y_test)\n",
    "        y_prob_all.append(y_prob)\n",
    "\n",
    "    # Abort if no valid folds produced predictions\n",
    "    if not y_true_all:\n",
    "        print(\"No CV folds produced predictions (check class balance per fold / source_file assignment).\")\n",
    "        return\n",
    "\n",
    "    # Pool all out-of-fold predictions\n",
    "    y_true_all = np.concatenate(y_true_all)\n",
    "    y_prob_all = np.concatenate(y_prob_all)\n",
    "    y_pred_all = (y_prob_all >= 0.5).astype(int)\n",
    "\n",
    "    # Print ONE combined result, in the style you want  #CHANGES MADE#\n",
    "    # Print a single combined evaluation across all folds\n",
    "    print(f\"Test rows: {len(y_true_all)}  Positives: {int(y_true_all.sum())}  Negatives: {int(len(y_true_all)-y_true_all.sum())}\")\n",
    "\n",
    "    if len(np.unique(y_true_all)) > 1:\n",
    "        print(\"AUROC:\", round(roc_auc_score(y_true_all, y_prob_all), 3))\n",
    "        print(\"AUPRC:\", round(average_precision_score(y_true_all, y_prob_all), 3))\n",
    "    print(\"Brier:\", round(brier_score_loss(y_true_all, y_prob_all), 4))\n",
    "    print(classification_report(y_true_all, y_pred_all, digits=3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb78d9-d7a1-4a7d-a65c-7b447195303a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
